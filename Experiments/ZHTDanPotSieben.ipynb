{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5189e182-d607-422d-8349-98ed66328ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import argparse\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import random\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchsummary import summary\n",
    "\n",
    "import bqplot.scales\n",
    "import ipyvolume as ipv\n",
    "import ipywidgets as widgets\n",
    "\n",
    "from collections import OrderedDict\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") \n",
    "print(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc41cea-6f52-4ff3-9085-ef406fb9b03a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def voxel_to_coordinates(voxel_data, voxel_size=0.01, threshold = 0):\n",
    "    xx, yy, zz = voxel_data.shape\n",
    "    x_coords = []\n",
    "    y_coords = []\n",
    "    z_coords = []\n",
    "    tags = []\n",
    "    for x in range(xx):\n",
    "        for y in range(yy):\n",
    "            for z in range(zz):\n",
    "                tag = voxel_data[x, y, z]\n",
    "                if tag > threshold:\n",
    "                    x_coords.append(x * voxel_size)\n",
    "                    y_coords.append(y * voxel_size)\n",
    "                    z_coords.append(z * voxel_size)\n",
    "                    tags.append(tag)\n",
    "\n",
    "    return np.array(x_coords), np.array(y_coords), np.array(z_coords), np.array(tags)\n",
    "\n",
    "def visualize_labeled_array3d(voxel_data, num_classes=7, size = None, marker = None):\n",
    "        voxel_data = voxel_data.astype(np.uint16)\n",
    "        x, y, z, tags = voxel_to_coordinates(voxel_data, voxel_size = 1 / voxel_data.shape[0])\n",
    "        color_scale = bqplot.scales.ColorScale(min=0, max=num_classes, colors=[\"#f00\", \"#00f\", \"#000000\", \"#808080\", \"#0f0\", \"#800080\"])\n",
    "        fig = ipv.figure()\n",
    "        unique_tags = np.unique(tags)\n",
    "\n",
    "        for tag in unique_tags:\n",
    "            mask = tags == tag\n",
    "            x_filtered, y_filtered, z_filtered, tags_f = x[mask], y[mask], z[mask], tags[mask]\n",
    "            \n",
    "            ipv.scatter(1-y_filtered,x_filtered, z_filtered, color=tags_f, color_scale=color_scale, marker=marker or 'box', size=size or 0.1, description=\"len({})={}\".format(str(tag),x_filtered.shape[0]))\n",
    "        ipv.xyzlabel('y','x','z')\n",
    "        ipv.view(0, -50, distance=2.5)\n",
    "        ipv.show()\n",
    "\n",
    "def plot_tensor2d(img_tensor):\n",
    "    tensor = img_tensor.permute(1, 2, 0)\n",
    "    tensor = (tensor - tensor.min()) / (tensor.max() - tensor.min())\n",
    "    plt.imshow(tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03623bd0-a2d5-4180-a1e1-ad7c2162ce21",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ab5e812-e297-48f2-93cc-cec4c4d1a088",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(\"/workspace/HKU-OccNet/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab58e258-149a-41fa-8546-c0182dabb630",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81e42e67-3db0-42fd-8d94-e4261931d61e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.calib_utils import *\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import h5py\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class PreprocessedDataset(Dataset):\n",
    "    def __init__(self, file_path):\n",
    "        self.file_path = file_path\n",
    "        with h5py.File(self.file_path, 'r') as hf:\n",
    "            self.length = len(hf.keys()) // 3  # Assuming each sample has 3 keys (l/r images and label)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        with h5py.File(self.file_path, 'r') as hf:\n",
    "            left_img = hf[f'left_image_{idx}'][:]\n",
    "            right_img = hf[f'right_image_{idx}'][:]\n",
    "            gt = hf[f'gt_{idx}'][:]\n",
    "            return torch.from_numpy(left_img), torch.from_numpy(right_img), torch.from_numpy(gt)\n",
    "\n",
    "img_width, img_height = 1241, 376\n",
    "calib = read_calib(\"/workspace/HKU-OccNet/calib.txt\")\n",
    "calib_proj = get_projections(img_width, img_height, calib)\n",
    "vox_origin = torch.tensor([0, 128, 10])\n",
    "\n",
    "vf_mask = calib_proj['fov_mask_1'].view(256, 256, 32)\n",
    "prj_pix = calib_proj['projected_pix_1'].view(256, 256, 32, 2)\n",
    "pix_z = calib_proj['pix_z_1'].view(256, 256, 32)\n",
    "\n",
    "cull_mask = torch.zeros((256, 256, 32)).bool()\n",
    "cull_mask[:int(0.5 * 256), :, :] = True\n",
    "\n",
    "\n",
    "\n",
    "#train_set = PreprocessedDataset('../preprocessed_data/preprocessed_999_1.h5')\n",
    "train_set = PreprocessedDataset('../preprocessed_data/preprocessed_500_10.h5')\n",
    "\n",
    "def rsplit(set, perc=0.7):\n",
    "    split_idx = int(perc * len(set))\n",
    "    return random_split(set, [split_idx, len(set) - split_idx])\n",
    "\n",
    "train_set_small, _ = rsplit(train_set, 0.75)\n",
    "train_subset, val_subset = rsplit(train_set_small, 0.98)\n",
    "\n",
    "train_dataloader = DataLoader(train_subset, batch_size=batch_size, shuffle=True)\n",
    "val_dataloader = DataLoader(val_subset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "\n",
    "print(len(train_dataloader))\n",
    "print(len(val_dataloader))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68d4d4e9-1ac1-47eb-b0ae-7260fd6f12e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# left_img, _, _ = train_set.__getitem__(450)\n",
    "# left_img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d60f9a60-6af5-47df-bf16-da828a35fa34",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eb27d35-dce7-48a7-9397-17543bb701d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b27ad6-89e5-4f1a-a917-069c128b4927",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sem_scal_loss, geo_scal_loss, CE_ssc_loss\n",
    "\n",
    "def geo_scal_loss(pred, ssc_target, epsilon=1e-6):\n",
    "    # Get softmax probabilities\n",
    "    pred = F.softmax(pred, dim=1)\n",
    "\n",
    "    # Check if prediction matches target exactly (pseudo-prediction case)\n",
    "    if torch.all(pred.argmax(dim=1) == ssc_target):\n",
    "        return torch.tensor(0.0).to(pred.device)\n",
    "\n",
    "    # Compute empty and nonempty probabilities\n",
    "    empty_probs = pred[:, 0, :, :, :]\n",
    "    nonempty_probs = 1 - empty_probs\n",
    "\n",
    "    # Remove unknown voxels (if necessary)\n",
    "    mask = ssc_target != 255\n",
    "    nonempty_target = ssc_target != 0\n",
    "    nonempty_target = nonempty_target[mask].float()\n",
    "    nonempty_probs = nonempty_probs[mask]\n",
    "    empty_probs = empty_probs[mask]\n",
    "\n",
    "    intersection = (nonempty_target * nonempty_probs).sum()\n",
    "    precision = intersection / (nonempty_probs.sum() + epsilon)\n",
    "    recall = intersection / (nonempty_target.sum() + epsilon)\n",
    "    spec = ((1 - nonempty_target) * (empty_probs)).sum() / ((1 - nonempty_target).sum() + epsilon)\n",
    "\n",
    "    return (\n",
    "        F.binary_cross_entropy(precision, torch.ones_like(precision))\n",
    "        + F.binary_cross_entropy(recall, torch.ones_like(recall))\n",
    "        + F.binary_cross_entropy(spec, torch.ones_like(spec))\n",
    "    )\n",
    "\n",
    "\n",
    "def sem_scal_loss(pred, ssc_target, epsilon=1e-6):\n",
    "    # Check for perfect match\n",
    "    if torch.all(pred.argmax(dim=1) == ssc_target):\n",
    "        return torch.tensor(0.0).to(pred.device)\n",
    "\n",
    "    # Get softmax probabilities\n",
    "    pred = F.softmax(pred, dim=1)\n",
    "    loss = 0\n",
    "    count = 0\n",
    "    mask = ssc_target != 255\n",
    "    n_classes = pred.shape[1]\n",
    "    for i in range(n_classes):\n",
    "        # Get probability of class i\n",
    "        p = pred[:, i, :, :, :][mask]\n",
    "        target = ssc_target[mask]\n",
    "\n",
    "        completion_target = (target == i).float()\n",
    "\n",
    "        if completion_target.sum() > 0:\n",
    "            count += 1.0\n",
    "            nominator = (p * completion_target).sum()\n",
    "\n",
    "            # Precision\n",
    "            precision = nominator / (p.sum() + epsilon)\n",
    "            loss_precision = F.binary_cross_entropy(\n",
    "                precision, torch.ones_like(precision)\n",
    "            )\n",
    "\n",
    "            # Recall\n",
    "            recall = nominator / (completion_target.sum() + epsilon)\n",
    "            loss_recall = F.binary_cross_entropy(recall, torch.ones_like(recall))\n",
    "\n",
    "            # Specificity\n",
    "            specificity = ((1 - p) * (1 - completion_target)).sum() / (\n",
    "                (1 - completion_target).sum() + epsilon\n",
    "            )\n",
    "            loss_specificity = F.binary_cross_entropy(\n",
    "                specificity, torch.ones_like(specificity)\n",
    "            )\n",
    "\n",
    "            loss_class = loss_precision + loss_recall + loss_specificity\n",
    "            loss += loss_class\n",
    "\n",
    "    return loss / max(count, epsilon)\n",
    "\n",
    "def CE_ssc_loss(pred, target, class_weights):\n",
    "    # Check for perfect match\n",
    "    if torch.all(pred.argmax(dim=1) == target):\n",
    "        return torch.tensor(0.0).to(pred.device)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss(\n",
    "        weight=class_weights, ignore_index=255, reduction=\"none\"\n",
    "    )\n",
    "    loss = criterion(pred, target.long())\n",
    "    loss_valid = loss[target != 255]\n",
    "    return torch.mean(loss_valid)\n",
    "\n",
    "\n",
    "\n",
    "def one_hot_encoding(labels, num_classes):\n",
    "    # Assuming labels of shape [batch_size, depth, height, width]\n",
    "    one_hot = F.one_hot(labels, num_classes)  # Convert to one-hot\n",
    "    return one_hot.permute(0, 4, 1, 2, 3).float()\n",
    "\n",
    "\n",
    "class_names = [\n",
    "            \"empty\", #0\n",
    "            \"vehicles\", #1\n",
    "            \"building\", #2\n",
    "            \"road\", #3\n",
    "            \"sidewalk\", #4\n",
    "            \"vegetation\", #5\n",
    "            \"others\", #6\n",
    "            \"unknown\", #7\n",
    "        ]\n",
    "num_classes = len(class_names)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ddeffdc-2ccc-4cd8-a2a3-374918d9911d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import load_STTR_model\n",
    "from utils import NestedTensor, batched_index_select\n",
    "\n",
    "pretrained_weight_path = '/workspace/PretrainedWeights/kitti_finetuned_model.pth.tar'\n",
    "\n",
    "class STTR_InputAdapterLayer(nn.Module):\n",
    "    def __init__(self, downsample=3):\n",
    "        super(STTR_InputAdapterLayer, self).__init__()\n",
    "        self.downsample = downsample\n",
    "\n",
    "    def forward(self, left_imgs, right_imgs):\n",
    "        bs, _, h, w = left_imgs.shape  # Extract batch size, height, and width\n",
    "\n",
    "        col_offset = int(self.downsample / 2)\n",
    "        row_offset = int(self.downsample / 2)\n",
    "        sampled_cols = torch.arange(col_offset, w, self.downsample)[None,].expand(bs, -1).cuda()\n",
    "        sampled_rows = torch.arange(row_offset, h, self.downsample)[None,].expand(bs, -1).cuda()\n",
    "        \n",
    "\n",
    "        # Create NestedTensor for the batch\n",
    "        nested_tensor = NestedTensor(left_imgs, right_imgs,  \n",
    "                                    sampled_cols=sampled_cols, sampled_rows=sampled_rows)\n",
    "\n",
    "        return nested_tensor\n",
    "\n",
    "class STTR(nn.Module):\n",
    "    def __init__(self, pretrained_weight_path, requires_grad = False):\n",
    "        super(STTR, self).__init__()\n",
    "        self.sttr_adapter_layer = STTR_InputAdapterLayer(downsample=3)\n",
    "        self.sttr_pt = load_STTR_model(pretrained_weight_path)\n",
    "        for param in self.sttr_pt.parameters():\n",
    "            param.requires_grad = requires_grad\n",
    "        \n",
    "    def forward(self, left_img, right_img): \n",
    "        x = self.sttr_adapter_layer(left_img, right_img)\n",
    "\n",
    "        # bs, _, h, w = x.left.size()\n",
    "        # feat = self.sttr_pt.backbone(x)\n",
    "        # tokens = self.sttr_pt.tokenizer(feat)\n",
    "        # pos_enc = self.sttr_pt.pos_encoder(x)\n",
    "        # # separate left and right\n",
    "        # feat_left = tokens[:bs]\n",
    "        # feat_right = tokens[bs:]  # NxCxHxW\n",
    "        # # downsample\n",
    "        # if x.sampled_cols is not None:\n",
    "        #     feat_left = batched_index_select(feat_left, 3, x.sampled_cols)\n",
    "        #     feat_right = batched_index_select(feat_right, 3, x.sampled_cols)\n",
    "        # if x.sampled_rows is not None:\n",
    "        #     feat_left = batched_index_select(feat_left, 2, x.sampled_rows)\n",
    "        #     feat_right = batched_index_select(feat_right, 2, x.sampled_rows)\n",
    "        # attn_weight = self.sttr_pt.transformer(feat_left, feat_right, pos_enc)\n",
    "        # output = self.sttr_pt.regression_head(attn_weight, x)\n",
    "\n",
    "        output = self.sttr_pt(x)\n",
    "        disp_map = output['disp_pred']\n",
    "        occ_map = output['occ_pred'] > 0.5\n",
    "        disp_map[occ_map] = 0.0\n",
    "        \n",
    "        return disp_map #, feat_left, feat_right, attn_weight\n",
    "\n",
    "\n",
    "sttr = STTR(pretrained_weight_path)\n",
    "print(\"Number of parameters (in millions):\", sum(p.numel() for p in sttr.parameters()) / 1_000_000, 'M')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a4c5e0-94e0-41c0-835b-38d719f2246e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import SegformerFeatureExtractor, SegformerForSemanticSegmentation\n",
    "from PIL import Image\n",
    "import requests\n",
    "\n",
    "class SegFormer(nn.Module):\n",
    "    def __init__(self, requires_grad=False):\n",
    "        super(SegFormer, self).__init__()\n",
    "        \n",
    "        self.feature_extractor = SegformerFeatureExtractor.from_pretrained(\"nvidia/segformer-b5-finetuned-cityscapes-1024-1024\")\n",
    "        self.segformer = SegformerForSemanticSegmentation.from_pretrained(\"nvidia/segformer-b5-finetuned-cityscapes-1024-1024\")\n",
    "        for param in self.segformer.parameters():\n",
    "            param.requires_grad = requires_grad\n",
    "        \n",
    "    def forward(self, img): \n",
    "        inputs = self.feature_extractor(images=img, return_tensors=\"pt\", do_rescale=False)\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "        hidden_state = self.segformer.segformer(**inputs).last_hidden_state      \n",
    "        logits = self.segformer(**inputs).logits\n",
    "        logits = F.interpolate(logits, size=(376, 1241), mode='bilinear', align_corners=False)\n",
    "        return logits, hidden_state\n",
    "segformer = SegFormer()\n",
    "print(\"Number of parameters (in millions):\", sum(p.numel() for p in segformer.parameters()) / 1_000_000, 'M')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e75ce91b-482a-4f49-9989-c1d05e28e67f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class Header(nn.Module):\n",
    "    def __init__(self, input_channel, num_classes):\n",
    "        super(Header, self).__init__()\n",
    "        self.C = input_channel\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        # Upsample layer to double the spatial dimensions\n",
    "        self.up_scale_2 = nn.Upsample(scale_factor=2, mode='trilinear', align_corners=True)\n",
    "\n",
    "        # Convolutional layers\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv3d(self.C, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm3d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv3d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm3d(128),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        # Final convolutional layer to produce logits for each class\n",
    "        self.final_conv = nn.Conv3d(128, self.num_classes, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x is expected to have the shape [B, C, 32, 32, 4]\n",
    "        \n",
    "        # Upscale to double the spatial dimensions: [B, C, 64, 64, 8]\n",
    "        x = self.up_scale_2(x)\n",
    "\n",
    "        # Pass through convolutional layers\n",
    "        x = self.conv_layers(x)\n",
    "\n",
    "        # Get num_classes for each voxel\n",
    "        x = self.final_conv(x)\n",
    "\n",
    "        # The output tensor shape will be [B, num_classes, 64, 64, 8]\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0776d692-eb69-4632-a3d1-1dc5c689e1a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa380187-5cce-4fc8-b5fa-d834d6f7811e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "864441e0-b1f2-498f-a977-7c8f3585b328",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads):\n",
    "        super(CrossAttention, self).__init__()\n",
    "        self.multihead_attn = nn.MultiheadAttention(embed_dim=embed_dim, num_heads=num_heads)\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "\n",
    "    def forward(self, query, key_value):\n",
    "        # Cross-Attention\n",
    "        attn_output, _ = self.multihead_attn(query, key_value, key_value)\n",
    "        # Add & Norm (Residual Connection and Layer Normalization)\n",
    "        query = self.norm1(query + attn_output)\n",
    "        return query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea13c3d3-e814-441c-a98f-73b9e88bb9b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import init\n",
    "import math \n",
    "class Conv3D_Block(nn.Module):\n",
    "    def __init__(self, ch_in, ch_out, k_size, stride=1, p=1):\n",
    "        super(Conv3D_Block, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv3d(ch_in, ch_out, kernel_size=k_size, stride=stride, padding=p),  \n",
    "            nn.BatchNorm3d(ch_out),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        out = self.conv(x)\n",
    "        return out\n",
    "\n",
    "class ResNet3D_Block(nn.Module):\n",
    "    def __init__(self, ch, k_size, stride=1, p=1):\n",
    "        super(ResNet3D_Block, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv3d(ch, ch, kernel_size=k_size, stride=stride, padding=p), \n",
    "            nn.BatchNorm3d(ch),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.Conv3d(ch, ch, kernel_size=k_size, stride=stride, padding=p),  \n",
    "            nn.BatchNorm3d(ch),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        out = self.conv(x) + x\n",
    "        return out\n",
    "\n",
    "# class ConvUpSample3D_Block(nn.Module):\n",
    "#     def __init__(self, ch_in, ch_out, k_size=1, scale=2, align_corners=False):\n",
    "#         super(ConvUpSample3D_Block, self).__init__()\n",
    "#         self.up = nn.Sequential(\n",
    "#             nn.Conv3d(ch_in, ch_out, kernel_size=k_size),\n",
    "#             nn.Upsample(scale_factor=scale, mode='trilinear', align_corners=align_corners),\n",
    "#         )\n",
    "#     def forward(self, x):\n",
    "#         return self.up(x)\n",
    "\n",
    "class Conv2D_Block(nn.Module):\n",
    "    def __init__(self, ch_in, ch_out, k_size, stride=1, p=1):\n",
    "        super(Conv2D_Block, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(ch_in, ch_out, kernel_size=k_size, stride=stride, padding=p),\n",
    "            nn.BatchNorm2d(ch_out),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv(x)\n",
    "        return out\n",
    "\n",
    "class ResNet2D_Block(nn.Module):\n",
    "    def __init__(self, ch, k_size, stride=1, p=1):\n",
    "        super(ResNet2D_Block, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(ch, ch, kernel_size=k_size, stride=stride, padding=p),\n",
    "            nn.BatchNorm2d(ch),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "\n",
    "            nn.Conv2d(ch, ch, kernel_size=k_size, stride=stride, padding=p),\n",
    "            nn.BatchNorm2d(ch),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv(x) + x\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8550f5eb-2ace-4d1b-8f68-fcd70a2e58b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from deformable_attention import DeformableAttention3D\n",
    "\n",
    "\n",
    "class DepthSegLift_OCC(nn.Module):\n",
    "    def __init__(self, num_classes, sttr, segformer):\n",
    "        super(DepthSegLift_OCC, self).__init__()\n",
    "\n",
    "        self.sttr = sttr\n",
    "        self.seg = segformer\n",
    "\n",
    "        ch_in = 26\n",
    "        ch_out = 64\n",
    "        self.ConvResBlock1 = nn.Sequential(\n",
    "            Conv2D_Block(ch_in=ch_in, ch_out=ch_out, k_size=3),\n",
    "            ResNet2D_Block(ch=ch_out, k_size=3),\n",
    "            nn.MaxPool2d(3, stride=4, padding=1)\n",
    "        )\n",
    "        ch_in = 64\n",
    "        ch_out = 128\n",
    "        self.ConvResBlock2 = nn.Sequential(\n",
    "            Conv2D_Block(ch_in=ch_in, ch_out=ch_out, k_size=3),\n",
    "            ResNet2D_Block(ch=ch_out, k_size=3),\n",
    "            nn.MaxPool2d(3, stride=4, padding=1)\n",
    "        )\n",
    "        ch_in = 128\n",
    "        ch_out = 256\n",
    "        self.ConvResBlock3 = nn.Sequential(\n",
    "            Conv2D_Block(ch_in=ch_in, ch_out=ch_out, k_size=3),\n",
    "            ResNet2D_Block(ch=ch_out, k_size=3),\n",
    "            nn.MaxPool2d(3, stride=2, padding=1)\n",
    "        )\n",
    "\n",
    "        self.hs_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=512, out_channels=256, kernel_size=1, stride=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "        self.relu = nn.LeakyReLU()\n",
    "        self.linear1 = nn.Linear(512*12*39, 128)\n",
    "        self.linear2 = nn.Linear(128, 32*32*32*4)\n",
    "\n",
    "        self.deform_attn3d = DeformableAttention3D(\n",
    "            dim = 32,                          # feature dimensions\n",
    "            dim_head = 32,                      # dimension per head\n",
    "            heads = 4,                          # attention heads\n",
    "            dropout = 0.5,                       # dropout\n",
    "            downsample_factor = (2, 8, 8),      # downsample factor (r in paper)\n",
    "            offset_scale = (2, 8, 8),           # scale of offset, maximum offset\n",
    "            offset_kernel_size = (4, 10, 10),   # offset kernel size\n",
    "        )\n",
    "\n",
    "        self.bn2d = nn.BatchNorm2d(512)\n",
    "        self.bn3d_att = nn.BatchNorm3d(32)\n",
    "        self.dropout1 = nn.Dropout(p=0.3)\n",
    "        self.dropout2 = nn.Dropout(p=0.2)\n",
    "        \n",
    "        self.header = Header(32, num_classes)\n",
    "        \n",
    "    def forward(self, left_img, right_img): \n",
    "        #disp, feat_left, feat_right, attn_weight = self.sttr(left_img, right_img)\n",
    "        #disp = self.sttr(left_img, right_img)\n",
    "\n",
    "        batch_size = left_img.size(0)\n",
    "        disp_list = []\n",
    "        for i in range(batch_size):\n",
    "            single_left_img = left_img[i].unsqueeze(0)  # Add batch dimension\n",
    "            single_right_img = right_img[i].unsqueeze(0)  # Add batch dimension\n",
    "            single_disp = self.sttr(single_left_img, single_right_img)\n",
    "            disp_list.append(single_disp)\n",
    "        disp = torch.cat(disp_list, dim=0)\n",
    "\n",
    "\n",
    "        \n",
    "        logit, hidden_state = self.seg(left_img)\n",
    "        x = torch.cat((disp.unsqueeze(1), logit, left_img, right_img), dim=1)\n",
    "        x = self.ConvResBlock1(x)\n",
    "        x = self.ConvResBlock2(x)\n",
    "        x = self.ConvResBlock3(x)\n",
    "\n",
    "        seg_hs = self.hs_conv(hidden_state)\n",
    "        seg_hs = self.relu(seg_hs)\n",
    "        seg_hs = F.adaptive_avg_pool2d(seg_hs, (12, 39))\n",
    "        x = torch.cat((x, seg_hs), dim=1)\n",
    "        x = self.bn2d(x)\n",
    "        x = self.relu(x)\n",
    "        x = torch.flatten(x, start_dim=1)\n",
    "\n",
    "        x = self.linear1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout1(x)\n",
    "        \n",
    "        x = self.linear2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout2(x)\n",
    "        \n",
    "        x = x.view(-1, 32, 32, 32, 4)\n",
    "        x_att = x.permute(0, 1, 4, 2, 3)\n",
    "        x_att = self.deform_attn3d(x_att)\n",
    "        x_att = x_att.permute(0, 1, 3, 4, 2)\n",
    "        x_att = self.bn3d_att(x_att)\n",
    "        x_att = self.relu(x_att)\n",
    "        \n",
    "        x = x + x_att\n",
    "\n",
    "        \n",
    "        \n",
    "        logit = self.header(x)\n",
    "        return logit\n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "    def step(self, left_img, right_img, gt, class_weights):\n",
    "        pred_1h = model(left_img, right_img)\n",
    "\n",
    "        loss = sem_scal_loss(pred_1h, gt)\n",
    "        loss += geo_scal_loss(pred_1h, gt)\n",
    "        loss += CE_ssc_loss(pred_1h, gt, class_weights)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be16bf9e-7728-42ee-bfd3-b1e087433929",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f86c2f6-88fe-4b16-812c-b077cef25630",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bb36b45-8ad7-47b9-8c38-42a67f6645b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed26783-0851-4fae-b9f8-9d4b3e7e7e95",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dba87d45-6970-4882-bc6c-c141a2debda0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2599f8f5-2871-46eb-8834-d82d8161515b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sttr = sttr.to(device)\n",
    "segformer = segformer.to(device)\n",
    "model = DepthSegLift_OCC(num_classes, sttr, segformer)\n",
    "model = model.to(device)\n",
    "print(\"Number of parameters (in millions):\", sum(p.numel() for p in model.parameters()) / 1_000_000, 'M')\n",
    "print(\"Number of trainable parameters (in millions):\", \n",
    "      sum(p.numel() for p in model.parameters() if p.requires_grad) / 1_000_000, 'M')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ebb80c-c945-4cb6-87f7-c5a3420812c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e82881-436b-494f-80a4-5315308ed481",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('dsocc_3.pth', map_location=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c31a31b-4164-43d5-982f-475d265f567e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "791e41c4-cdf5-4be4-8185-87f22d50e1ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eba3216b-d141-4929-87e4-4d1a4fb63b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weights  = torch.tensor([\n",
    "            0.2, #\"empty\", #0\n",
    "            0.15, #\"vehicles\", #1\n",
    "            0.125, #\"building\", #2\n",
    "            0.15, #\"road\", #3\n",
    "            0.12, #\"sidewalk\", #4\n",
    "            0.105, #\"vegetation\", #5\n",
    "            0.0999, #others #6\n",
    "            0.0001 #\"unknown\", #7\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6954ea12-7a5c-4c99-8339-b0defc89103a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Adam\n",
    "optimizer = Adam(model.parameters(), lr=1.5e-4, weight_decay=1e-4)\n",
    "\n",
    "best_loss = np.inf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b86d23d-cad4-4151-990f-acaa12e20457",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "\n",
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = 0.0\n",
    "    for i, (left_img, right_img, gt) in tqdm(enumerate(train_dataloader), total = len(train_dataloader)):\n",
    "        left_img = left_img.to(device)\n",
    "        right_img = right_img.to(device)\n",
    "        gt = gt.to(device)\n",
    "        class_weights = class_weights.to(device)\n",
    "        loss = model.step(left_img, right_img, gt, class_weights)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=20.0)\n",
    "        \n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "        #print(loss.item())  \n",
    "    valid_loss = 0.0\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for (left_img, right_img, gt) in tqdm(val_dataloader):\n",
    "            left_img = left_img.to(device)\n",
    "            right_img = right_img.to(device)\n",
    "            gt = gt.to(device)\n",
    "            class_weights = class_weights.to(device)\n",
    "            loss = model.step(left_img, right_img, gt, class_weights)\n",
    "            \n",
    "            # optimizer.zero_grad()\n",
    "            # loss.requires_grad_(True)\n",
    "            # loss.backward()\n",
    "            # optimizer.step()\n",
    "            valid_loss += loss.item()\n",
    "            \n",
    "    print(f'Epoch {epoch + 1}: Training loss: {train_loss / len(train_dataloader)}, Validation loss: {valid_loss / len(val_dataloader)}')\n",
    "    if (train_loss / len(train_dataloader)) < best_loss:\n",
    "        torch.save(model.state_dict(), f'dsocc_{epoch + 1}.pth')\n",
    "        best_loss = (train_loss / len(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fcd8189-40de-4250-b3bf-1049edcfb661",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c07cf694-3766-462a-9893-a20c86bf6990",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8caa3e58-acca-4aae-bd51-9c814921a145",
   "metadata": {},
   "outputs": [],
   "source": [
    "left_img, right_img, gt = val_subset.__getitem__(random.randint(0, len(val_subset)-1))\n",
    "\n",
    "left_img = left_img.unsqueeze(0)\n",
    "right_img = right_img.unsqueeze(0)\n",
    "gt = gt.unsqueeze(0)\n",
    "plot_tensor2d(left_img[0].cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8e91dce-3d5c-4e42-b9a1-b3273c726ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "left_img = left_img.to(device)\n",
    "right_img = right_img.to(device)\n",
    "gt = gt.to(device)\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    pred_1h = model(left_img, right_img)\n",
    "\n",
    "pred = torch.argmax(pred_1h, dim=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61b8246a-fb0e-4f49-8b84-ff5c491d7047",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_tensor2d(left_img[0].cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c78f9e95-9cdc-49c2-b5aa-10cd06e75547",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_labeled_array3d((pred.view(64, 64, 8).float()).detach().cpu().numpy(), size = 2, marker = 'box')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e4ad8b2-7bb6-4683-832d-1ab477bb82de",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_labeled_array3d((gt.view(64, 64, 8).float()).detach().cpu().numpy(), size = 2, marker = 'box')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6963645-5c20-4acb-a769-c9ecc6a59db8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff1f41db-5ef7-4505-99c8-4c797f224013",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dff2783-6ed6-436c-957a-498b121c019f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa59ed56-d2db-4ec2-8aeb-030ad82c5ca4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "251129f1-f2c9-4492-85a9-42fceb9d6e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = PreprocessedDataset('../preprocessed_data/preprocessed_500_10.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "589e0126-413f-4370-bc7f-1f04e12a0455",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "904baf2a-ad9d-46ad-8d5a-27ceb303f7c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "left_img, right_img, gt = test_set.__getitem__(random.randint(0, len(test_set)-1))\n",
    "\n",
    "left_img = left_img.unsqueeze(0)\n",
    "right_img = right_img.unsqueeze(0)\n",
    "gt = gt.unsqueeze(0)\n",
    "plot_tensor2d(left_img[0].cpu())\n",
    "\n",
    "\n",
    "left_img = left_img.to(device)\n",
    "right_img = right_img.to(device)\n",
    "gt = gt.to(device)\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    pred_1h = model(left_img, right_img)\n",
    "\n",
    "pred = torch.argmax(pred_1h, dim=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a01788d8-3e2d-4803-90a9-159e38cbc881",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_labeled_array3d((pred.view(64, 64, 8).float()).detach().cpu().numpy(), size = 2, marker = 'box')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c5cc519-487e-4964-bb8c-c354bfb4a396",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_labeled_array3d((gt.view(64, 64, 8).float()).detach().cpu().numpy(), size = 2, marker = 'box')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b91caa9-543c-4e07-804d-c30a7a01c36b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a483778-eaa0-4d59-82f1-d338a5852abf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70a16f38-b069-4ec0-bc4a-bbdff6d63879",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a11ee23a-5f12-43b0-a081-c0dd7eb08a66",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f24939d-2170-4cfd-877c-bb1a71183b29",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
