{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4baed1fd-fd63-4791-ba04-2a15db604bdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import argparse\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import random\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchsummary import summary\n",
    "\n",
    "import bqplot.scales\n",
    "import ipyvolume as ipv\n",
    "import ipywidgets as widgets\n",
    "\n",
    "from collections import OrderedDict\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") \n",
    "print(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdea723f-d3de-4e43-85e7-dcd5697c73f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63f56ef6-9f95-486a-87b3-6876ea44362c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e3ff26-1337-48c2-b329-a2a9e14ff151",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "efbfa2f8-b984-4855-9136-7c96ae21bb9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpack(compressed):\n",
    "  ''' given a bit encoded voxel grid, make a normal voxel grid out of it.  '''\n",
    "  uncompressed = np.zeros(compressed.shape[0] * 8, dtype=np.uint8)\n",
    "  uncompressed[::8] = compressed[:] >> 7 & 1\n",
    "  uncompressed[1::8] = compressed[:] >> 6 & 1\n",
    "  uncompressed[2::8] = compressed[:] >> 5 & 1\n",
    "  uncompressed[3::8] = compressed[:] >> 4 & 1\n",
    "  uncompressed[4::8] = compressed[:] >> 3 & 1\n",
    "  uncompressed[5::8] = compressed[:] >> 2 & 1\n",
    "  uncompressed[6::8] = compressed[:] >> 1 & 1\n",
    "  uncompressed[7::8] = compressed[:] & 1\n",
    "\n",
    "  return uncompressed\n",
    "\n",
    "class KittiLoader(Dataset):\n",
    "    def __init__(self, root_dir, mode='train', sequences=['00'], split_ratio=1.0, \n",
    "                 get_vox_lidar=False, get_vox_invalid=False, get_vox_occluded=False):\n",
    "        self.root_dir = root_dir\n",
    "        self.mode = mode\n",
    "        self.sequences = sequences\n",
    "        self.image_names = []\n",
    "        self.image2_dir = {}\n",
    "        self.image3_dir = {}\n",
    "        self.voxels_dir = {}\n",
    "        self.get_vox_lidar=get_vox_lidar\n",
    "        self.get_vox_invalid=get_vox_invalid\n",
    "        self.get_vox_occluded=get_vox_occluded\n",
    "        \n",
    "        self.scene_size = [51.2, 51.2, 6.4] #unit m, 51.2m = 256 * 0.2m\n",
    "        self.vox_origin = [0, -25.6, -2]#unit m\n",
    "        self.voxel_size = 0.2  #m\n",
    "\n",
    "        for seq in self.sequences:\n",
    "            self.image2_dir[seq] = os.path.join(root_dir, 'sequences', seq, 'image_2')\n",
    "            self.image3_dir[seq] = os.path.join(root_dir, 'sequences', seq, 'image_3')\n",
    "            self.voxels_dir[seq] = os.path.join(root_dir, 'sequences', seq, 'voxels')\n",
    "            seq_image_names = [f.split('.')[0] for f in os.listdir(self.image2_dir[seq]) if f.endswith('.png')]\n",
    "            self.image_names.extend([(seq, img_name) for img_name in seq_image_names])\n",
    "        \n",
    "        split_idx = int(len(self.image_names) * split_ratio )\n",
    "        \n",
    "        if self.mode == 'train':\n",
    "            self.image_names = self.image_names[:split_idx]\n",
    "        else:\n",
    "            self.image_names = self.image_names[split_idx:]\n",
    "        \n",
    "        # Define transformations for standardization and conversion to tensor\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406],  # Standard ImageNet mean\n",
    "                                 std=[0.229, 0.224, 0.225])  # Standard ImageNet std\n",
    "        ])\n",
    "\n",
    "        self.class_map = {\n",
    "            0 : 0,     # \"unlabeled\"\n",
    "            1 : 0,     # \"outlier\" mapped to \"unlabeled\" --------------------------mapped\n",
    "            10: 1,     # \"car\"\n",
    "            11: 1,     # \"bicycle\"\n",
    "            13: 1,     # \"bus\" mapped to \"other-vehicle\" --------------------------mapped\n",
    "            15: 1,     # \"motorcycle\"\n",
    "            16: 1,     # \"on-rails\" mapped to \"other-vehicle\" ---------------------mapped\n",
    "            18: 1,     # \"truck\"\n",
    "            20: 1,     # \"other-vehicle\"\n",
    "            30: 1,     # \"person\"\n",
    "            31: 1,     # \"bicyclist\"\n",
    "            32: 1,     # \"motorcyclist\"\n",
    "            40: 3,     # \"road\"\n",
    "            44: 2,    # \"parking\"\n",
    "            48: 4,    # \"sidewalk\"\n",
    "            49: 4,    # \"other-ground\"\n",
    "            50: 2,    # \"building\"\n",
    "            51: 2,    # \"fence\"\n",
    "            52: 2,     # \"other-structure\" mapped to \"unlabeled\" ------------------mapped ####\n",
    "            60: 3,     # \"lane-marking\" to \"road\" ---------------------------------mapped\n",
    "            70: 5,    # \"vegetation\"\n",
    "            71: 5,    # \"trunk\"\n",
    "            72: 3,    # \"terrain\"\n",
    "            80: 2,    # \"pole\"\n",
    "            81: 2,    # \"traffic-sign\"\n",
    "            99: 1,     # \"other-object\" to \"unlabeled\" ----------------------------mapped ####\n",
    "            252: 1,    # \"moving-car\" to \"car\" ------------------------------------mapped\n",
    "            253: 1,    # \"moving-bicyclist\" to \"bicyclist\" ------------------------mapped\n",
    "            254: 1,    # \"moving-person\" to \"person\" ------------------------------mapped\n",
    "            255: 1,    # \"moving-motorcyclist\" to \"motorcyclist\" ------------------mapped\n",
    "            256: 1,    # \"moving-on-rails\" mapped to \"other-vehicle\" --------------mapped\n",
    "            257: 1,    # \"moving-bus\" mapped to \"other-vehicle\" -------------------mapped\n",
    "            258: 1,    # \"moving-truck\" to \"truck\" --------------------------------mapped\n",
    "            259: 1,    # \"moving-other\"-vehicle to \"other-vehicle\" ----------------mapped\n",
    "        }\n",
    "        \n",
    "        self.class_names = [\n",
    "            \"empty\", #0\n",
    "            \"obstacles\", #1\n",
    "            \"building\", #2\n",
    "            \"road\", #3\n",
    "            \"sidewalk\", #4\n",
    "            \"vegetation\", #5\n",
    "            \"unknown\", #6\n",
    "        ]\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_names)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        seq, img_name = self.image_names[idx]\n",
    "        img2_name = os.path.join(self.image2_dir[seq], img_name + '.png')\n",
    "        img3_name = os.path.join(self.image3_dir[seq], img_name + '.png')\n",
    "        if self.get_vox_lidar:\n",
    "            voxel_name = os.path.join(self.voxels_dir[seq], img_name + '.bin')\n",
    "        label_name = os.path.join(self.voxels_dir[seq], img_name + '.label')\n",
    "        if self.get_vox_invalid:\n",
    "            invalid_name = os.path.join(self.voxels_dir[seq], img_name + '.invalid')\n",
    "        if self.get_vox_occluded:\n",
    "            occluded_name = os.path.join(self.voxels_dir[seq], img_name + '.occluded')\n",
    "        \n",
    "        \n",
    "        # Load and transform images\n",
    "        left_image = self.transform(Image.open(img2_name).convert('RGB'))\n",
    "        right_image = self.transform(Image.open(img3_name).convert('RGB'))\n",
    "\n",
    "        extra_data = []\n",
    "        # Load voxel data and convert to tensor\n",
    "        if self.get_vox_lidar:\n",
    "            voxel_data = torch.tensor(unpack(np.fromfile(voxel_name, dtype=np.uint8)).reshape(256, 256, 32).astype(np.float32))\n",
    "            extra_data.append(voxel_data)\n",
    "        #  0 invalid 1-19 class and 255 invalid\n",
    "        voxel_labels = np.fromfile(label_name, dtype=np.uint16).reshape(256, 256, 32).astype(np.float32)\n",
    "        voxel_labels = torch.tensor(np.vectorize(self.class_map.get)(voxel_labels))\n",
    "        #print(voxel_data.shape)\n",
    "        #print(voxel_labels.shape)\n",
    "        \n",
    "        if self.get_vox_occluded:\n",
    "            voxel_occluded = torch.tensor(unpack(np.fromfile(occluded_name, dtype=np.uint8)).reshape(256, 256, 32).astype(np.float32))\n",
    "            extra_data.append(voxel_occluded)\n",
    "        \n",
    "        if self.get_vox_invalid:\n",
    "            voxel_invalid = torch.tensor(unpack(np.fromfile(invalid_name, dtype=np.uint8)).reshape(256, 256, 32).astype(np.float32))\n",
    "            extra_data.append(voxel_invalid)\n",
    "        \n",
    "        \n",
    "        \n",
    "        if len(extra_data) != 0:\n",
    "            return left_image, right_image, voxel_labels, tuple(extra_data)\n",
    "        else:\n",
    "            return left_image, right_image, voxel_labels\n",
    "    \n",
    "    \n",
    "    def get_data(self, idx):\n",
    "        return self.__getitem__(idx)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "90d1f5eb-eaa3-4cce-9572-0e3109ee68d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def voxel_to_coordinates(voxel_data, voxel_size=0.01, threshold = 0):\n",
    "    # 获取体素数据的形状\n",
    "    xx, yy, zz = voxel_data.shape\n",
    "\n",
    "    # 初始化坐标和标签列表\n",
    "    x_coords = []\n",
    "    y_coords = []\n",
    "    z_coords = []\n",
    "    tags = []\n",
    "\n",
    "    # 遍历体素数据\n",
    "    for x in range(xx):\n",
    "        for y in range(yy):\n",
    "            for z in range(zz):\n",
    "                tag = voxel_data[x, y, z]\n",
    "                # 只记录非零标签的体素\n",
    "                if tag > threshold:\n",
    "                    x_coords.append(x * voxel_size)\n",
    "                    y_coords.append(y * voxel_size)\n",
    "                    z_coords.append(z * voxel_size)\n",
    "                    tags.append(tag)\n",
    "\n",
    "    return np.array(x_coords), np.array(y_coords), np.array(z_coords), np.array(tags)\n",
    "\n",
    "def visualize_labeled_array3d(voxel_data, num_classes=7, size = None, marker = None):\n",
    "        voxel_data = voxel_data.astype(np.uint16)\n",
    "        x, y, z, tags = voxel_to_coordinates(voxel_data, voxel_size = 1 / voxel_data.shape[0])\n",
    "\n",
    "\n",
    "        # 创建颜色比例尺\n",
    "        color_scale = bqplot.scales.ColorScale(min=0, max=num_classes, colors=[\"#f00\", \"#00f\", \"#000000\", \"#808080\", \"#0f0\", \"#800080\"])\n",
    "\n",
    "        fig = ipv.figure()\n",
    "\n",
    "        # 确定tags中的唯一值\n",
    "        unique_tags = np.unique(tags)\n",
    "\n",
    "        # 为每个唯一的tag值创建一个scatter\n",
    "        for tag in unique_tags:\n",
    "            # 过滤出当前tag的坐标\n",
    "            mask = tags == tag\n",
    "            x_filtered, y_filtered, z_filtered, tags_f = x[mask], y[mask], z[mask], tags[mask]\n",
    "            \n",
    "            ipv.scatter(1-y_filtered,x_filtered, z_filtered, color=tags_f, color_scale=color_scale, marker=marker or 'box', size=size or 0.1, description=\"len({})={}\".format(str(tag),x_filtered.shape[0]))\n",
    "        ipv.xyzlabel('y','x','z')\n",
    "        ipv.view(0, -50, distance=2.5)\n",
    "        ipv.show()\n",
    "\n",
    "def plot_tensor2d(img_tensor):\n",
    "    tensor = img_tensor.permute(1, 2, 0)\n",
    "    tensor = (tensor - tensor.min()) / (tensor.max() - tensor.min())\n",
    "    plt.imshow(tensor)\n",
    "\n",
    "def visualize_3D_points_in_jupyter(points_3D, size=2, marker=\"sphere\"):\n",
    "    # Assuming points_3D is a N x 3 numpy array\n",
    "    x = points_3D[:, 0]\n",
    "    y = points_3D[:, 1]\n",
    "    z = points_3D[:, 2]\n",
    "\n",
    "    ipv.quickscatter(x, y, z, size=size, marker=marker)\n",
    "    ipv.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54293df1-289d-43cb-8a54-314a65939a3f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db00cd1-4a29-42fd-8fcf-0fb11f715a2f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b93649d8-b77d-4611-a767-c516627133d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93558259-72dc-4c06-9bfc-ac78af22587e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e242f755-7327-4156-b5c9-1eba550646d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f61e83f4-fc0b-4c08-a820-0be1d799498d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a90817fd-4e4f-4adc-8cac-45d01512859d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8213ae6-34e2-42de-87d8-3ba1c7fd01d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4533b267-2c0f-4c24-b4c1-7c891a96dd79",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe5bbd51-c75c-4ffc-8896-259986f3cf21",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "737b2c0e-3231-4c5b-a175-9634083e9d9a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "654c44a3-000d-4c4c-9bed-673fea4edb0a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00436b9c-8cfe-4198-98db-519fb74f6e85",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1875d3d8-8b7d-403e-a950-8324e1e31659",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75536b19-45f9-4f10-bab3-6ff877f2648f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23a36465-9e31-4b84-aaa2-5fc8df2c8de7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "72f342d4-1aff-4395-9e27-9b4b6edd0abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(\"/workspace/HKU-OccNet/\")\n",
    "from utils.calib_utils import *\n",
    "img_width, img_height = 1241, 376\n",
    "calib = read_calib(\"/workspace/HKU-OccNet/calib.txt\")\n",
    "calib_proj = get_projections(img_width, img_height, calib)\n",
    "train_set = KittiLoader(root_dir=\"/workspace/Dataset2/dataset\", mode='train', \n",
    "                        sequences=['00', '01', '02'], \n",
    "                        get_vox_lidar=True, get_vox_invalid=True, get_vox_occluded=True)\n",
    "\n",
    "vox_origin = torch.tensor([0, 128, 10])\n",
    "\n",
    "vf_mask = calib_proj['fov_mask_1'].view(256, 256, 32)\n",
    "prj_pix = calib_proj['projected_pix_1'].view(256, 256, 32, 2)\n",
    "pix_z = calib_proj['pix_z_1'].view(256, 256, 32)\n",
    "\n",
    "cull_mask = torch.zeros((256, 256, 32)).bool()\n",
    "cull_mask[:int(0.5 * 256), :, :] = True\n",
    "\n",
    "# def preprocess_gt(voxel_labels, voxel_invalid, cull_mask, vf_mask):\n",
    "#     batch_size = voxel_labels.shape[0]\n",
    "#     cull_mask = cull_mask.unsqueeze(0).expand(batch_size, 256, 256, 32)\n",
    "#     vf_mask = vf_mask.unsqueeze(0).expand(batch_size, 256, 256, 32)\n",
    "#     v = voxel_labels.clone()\n",
    "#     v[(v == 0) & (voxel_invalid > 0.5)] = 20\n",
    "#     v[~vf_mask] = 0\n",
    "#     v = v * cull_mask\n",
    "#     return v.unsqueeze(1)\n",
    "\n",
    "# def preprocess_gt_bool(voxel_labels, voxel_invalid, mask = None):\n",
    "#     batch_size = voxel_labels.shape[0]\n",
    "#     v = voxel_labels.clone()\n",
    "#     v[(v == 0) & (voxel_invalid > 0.5)] = 6\n",
    "\n",
    "#     if mask != None:\n",
    "#         mask = mask.unsqueeze(0).expand(batch_size, 256, 256, 32)\n",
    "#         v[~mask] = 0\n",
    "        \n",
    "#     v = v.unsqueeze(1) > 0.5\n",
    "#     return v.float()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ad740581-34d9-4d3e-b415-2fb5c02d2ed5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10303\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(train_set))\n",
    "len(train_set.class_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "452e7b17-b8b8-4fa8-801f-266d47d46ea3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c593a394-6efc-4a8e-84a1-b227ffe1a4e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# left_img, right_img, voxel_labels, extra_labels = train_set.__getitem__(random.randint(0, len(train_set)))\n",
    "# voxel_lidar, voxel_occluded, voxel_invalid  = extra_labels\n",
    "\n",
    "\n",
    "# plot_tensor2d(left_img)\n",
    "# visualize_labeled_array3d((voxel_labels.view(256, 256, 32)).cpu().numpy(), num_classes=len(train_set.class_names), \n",
    "#                           size = 0.5, marker = 'box')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f4eee1-f149-4603-8558-835fad0bb195",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5b06ccc0-5a7c-49b0-8100-6d40c0ee1554",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_gt(voxel_labels, voxel_invalid, mask = None):\n",
    "    batch_size = voxel_labels.shape[0]\n",
    "    v = voxel_labels.clone()\n",
    "    v = v[:, int(0.1*256):int(0.6*256), 128-64:128+64, 0:16]\n",
    "    \n",
    "    #vi = voxel_invalid[:, int(0.1*256):int(0.6*256), 128-64:128+64, 0:16]\n",
    "    \n",
    "    #print(v.shape)\n",
    "    #v[(v == 0) & (voxel_occluded > 0.5)] = 6\n",
    "    \n",
    "    if mask != None:\n",
    "        mask = mask[int(0.1*256):int(0.6*256), 128-64:128+64, 0:16]\n",
    "        mask = mask.unsqueeze(0).expand(batch_size, 128, 128, 16)\n",
    "        v[~mask] = 0\n",
    "        \n",
    "    return v\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2d2a6db4-c59b-45b6-90bb-84f40b18d7ae",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6e5891275fb4edf9150a00377eb8dcb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Container(figure=Figure(box_center=[0.5, 0.5, 0.5], box_size=[1.0, 1.0, 1.0], camera=PerspectiveCamera(fov=45.…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "left_img, right_img, voxel_labels, extra_labels = train_set.__getitem__(random.randint(0, len(train_set)))\n",
    "voxel_lidar, voxel_occluded, voxel_invalid  = extra_labels\n",
    "\n",
    "voxel_pgt = preprocess_gt(voxel_labels.unsqueeze(0), voxel_invalid.unsqueeze(0), vf_mask)\n",
    "visualize_labeled_array3d((voxel_pgt.view(128, 128, 16)).cpu().numpy(), num_classes=len(train_set.class_names), \n",
    "                                  size = 1, marker = 'box')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9c527e13-2869-40b0-84bf-7246ee79d704",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot_tensor2d(left_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2b2489d7-5366-4f81-a2d3-cea53d1d0c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.unique(voxel_pgt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1825a3c-7bc3-4cbb-9fb2-9da6f5a8054c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "540d9577-1261-47ff-bff9-5a2c39d1e47f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "861af510-2173-489d-9bc0-e38cbeadb776",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ede9bb-4a28-4a4f-aa56-9005bd0b649e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff7dd767-000d-4179-8db8-3756d32b0059",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a343d204-bb32-46ce-a3f0-aa848d797c1f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5a819725-2d95-4407-b292-a806f7fe7d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# left_img, right_img, voxel_labels, extra_labels = train_set.__getitem__(random.randint(0, len(train_set)))\n",
    "# voxel_lidar, voxel_occluded, voxel_invalid  = extra_labels\n",
    "\n",
    "\n",
    "# plot_tensor2d(left_img)\n",
    "# voxel_pgt = preprocess_gt(voxel_labels.unsqueeze(0), voxel_invalid.unsqueeze(0), cull_mask, vf_mask)\n",
    "# visualize_labeled_array3d((voxel_pgt.view(256, 256, 32)).cpu().numpy(), size = 0.5, marker = 'box')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "455e001d-077d-45d1-8792-4275d6f29c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# voxel_pgt = preprocess_gt_bool(voxel_labels.unsqueeze(0), voxel_invalid.unsqueeze(0),  vf_mask  & cull_mask)\n",
    "# visualize_labeled_array3d((voxel_pgt.view(256, 256, 32)).cpu().numpy(), size = 0.5, marker = 'box')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9e085ac-146f-4266-acd8-782ccb3b4eaa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c54e9c-627f-4ec3-9069-fa97352c66ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "519aae5d-e585-47dd-a394-06598b09ab8d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bea2f7f-0560-428e-a996-1ef131bd9a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a00bc92-b19d-4dc5-9cde-f3cccdd6b172",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03b24d45-5901-4992-986d-08ac0e05cb97",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8be969e5-cb93-4ac8-b2bf-3311953f8111",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a942e50b-4b3f-45f7-b9c3-a0f8ab9675b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd129b83-bb48-47cc-a33b-2b59eef6b17a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21509175-2ba9-4ae3-92f4-c27c8694103d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "668a4099-7bf7-4cfa-8e6a-a36d90d0db08",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "73311cd2-cdca-45ea-900a-bc62c8e8c15d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reference: https://github.com/duyphuongcri/Variational-AutoEncoder/blob/master/model.py\n",
    "\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import init\n",
    "import math \n",
    "class Conv3D_Block(nn.Module):\n",
    "    def __init__(self, ch_in, ch_out, k_size, stride=1, p=1):\n",
    "        super(Conv3D_Block, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv3d(ch_in, ch_out, kernel_size=k_size, stride=stride, padding=p),  \n",
    "            nn.BatchNorm3d(ch_out),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        out = self.conv(x)\n",
    "        return out\n",
    "\n",
    "class ResNet3D_Block(nn.Module):\n",
    "    def __init__(self, ch, k_size, stride=1, p=1):\n",
    "        super(ResNet3D_Block, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv3d(ch, ch, kernel_size=k_size, stride=stride, padding=p), \n",
    "            nn.BatchNorm3d(ch),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.Conv3d(ch, ch, kernel_size=k_size, stride=stride, padding=p),  \n",
    "            nn.BatchNorm3d(ch),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        out = self.conv(x) + x\n",
    "        return out\n",
    "\n",
    "\n",
    "# class ConvUpSample3D_Block(nn.Module):\n",
    "#     def __init__(self, ch_in, ch_out, k_size=1, scale=2, align_corners=False):\n",
    "#         super(ConvUpSample3D_Block, self).__init__()\n",
    "#         self.up = nn.Sequential(\n",
    "#             nn.Conv3d(ch_in, ch_out, kernel_size=k_size),\n",
    "#             nn.Upsample(scale_factor=scale, mode='trilinear', align_corners=align_corners),\n",
    "#         )\n",
    "#     def forward(self, x):\n",
    "#         return self.up(x)\n",
    "\n",
    "class ConvUpSample3D_Block(nn.Module):\n",
    "    def __init__(self, ch_in, ch_out, k_size=3, stride=2, output_padding=1):\n",
    "        super(ConvUpSample3D_Block, self).__init__()\n",
    "        self.up = nn.Sequential(\n",
    "            nn.ConvTranspose3d(ch_in, ch_out, kernel_size=k_size, stride=stride, padding=1, output_padding=output_padding),\n",
    "            nn.BatchNorm3d(ch_out),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.up(x)\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, latent_dim):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.conv1 = Conv3D_Block(ch_in=1, ch_out=8, k_size=3)\n",
    "        self.res_block1 = ResNet3D_Block(ch=8, k_size=3)\n",
    "        self.MaxPool1 = nn.MaxPool3d(3, stride=2, padding=1)\n",
    "\n",
    "        self.conv2 = Conv3D_Block(ch_in=8, ch_out=16, k_size=3)\n",
    "        self.res_block2 = ResNet3D_Block(ch=16, k_size=3)\n",
    "        self.MaxPool2 = nn.MaxPool3d(3, stride=2, padding=1)\n",
    "\n",
    "        self.conv3 = Conv3D_Block(ch_in=16, ch_out=32, k_size=3)\n",
    "        self.res_block3 = ResNet3D_Block(ch=32, k_size=3)\n",
    "        self.MaxPool3 = nn.MaxPool3d(3, stride=2, padding=1)\n",
    "\n",
    "        self.conv4 = Conv3D_Block(ch_in=32, ch_out=64, k_size=3)\n",
    "        self.res_block4 = ResNet3D_Block(ch=64, k_size=3)\n",
    "        self.MaxPool4 = nn.MaxPool3d(3, stride=2, padding=1)\n",
    "\n",
    "        self.z_mean = nn.Linear(64*8*8*1, latent_dim)\n",
    "        self.z_log_sigma = nn.Linear(64*8*8*1, latent_dim)\n",
    "            \n",
    "        self.reset_parameters()\n",
    "      \n",
    "    def reset_parameters(self):\n",
    "        for weight in self.parameters():\n",
    "            if weight.dim() > 1:\n",
    "                torch.nn.init.kaiming_uniform_(weight)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.conv1(x)\n",
    "        x1 = self.res_block1(x1)\n",
    "        x1 = self.MaxPool1(x1) \n",
    "        #print(x1.shape) \n",
    "        \n",
    "        x2 = self.conv2(x1)\n",
    "        x2 = self.res_block2(x2)\n",
    "        x2 = self.MaxPool2(x2) \n",
    "        #print(x2.shape) \n",
    "\n",
    "        x3 = self.conv3(x2)\n",
    "        x3 = self.res_block3(x3)\n",
    "        x3 = self.MaxPool3(x3) \n",
    "        #print(x3.shape) \n",
    "        \n",
    "        x4 = self.conv4(x3)\n",
    "        x4 = self.res_block4(x4) \n",
    "        x4 = self.MaxPool4(x4) \n",
    "        #print(x4.shape) \n",
    "        x = x4\n",
    "\n",
    "        x = torch.flatten(x, start_dim=1)\n",
    "        \n",
    "        z_mean = self.z_mean(x)\n",
    "        z_log_sigma = self.z_log_sigma(x)\n",
    "\n",
    "        # 避免数值不稳定\n",
    "        z_log_sigma = torch.clamp(z_log_sigma, max=0) # 限制z_log_sigma的最大值\n",
    "        \n",
    "        \n",
    "        return z_mean, z_log_sigma\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, latent_dim, num_classes, device):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.device = device\n",
    "        \n",
    "        self.latent_dim = latent_dim\n",
    "        self.linear_up = nn.Linear(latent_dim, 64*8*8*1)  \n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        # Upsampling layers\n",
    "        self.upsample4 = ConvUpSample3D_Block(ch_in=64, ch_out=32, k_size=3, stride=2, output_padding=1)\n",
    "        self.res_block4 = ResNet3D_Block(ch=32, k_size=3)\n",
    "        \n",
    "        self.upsample3 = ConvUpSample3D_Block(ch_in=32, ch_out=16, k_size=3, stride=2, output_padding=1)\n",
    "        self.res_block3 = ResNet3D_Block(ch=16, k_size=3)        \n",
    "        \n",
    "        self.upsample2 = ConvUpSample3D_Block(ch_in=16, ch_out=8, k_size=3, stride=2, output_padding=1)\n",
    "        self.res_block2 = ResNet3D_Block(ch=8, k_size=3)   \n",
    "        \n",
    "        # Final upsampling to reach the desired output shape\n",
    "        self.upsample1 = ConvUpSample3D_Block(ch_in=8, ch_out=num_classes, k_size=3, stride=2, output_padding=1)\n",
    "        self.res_block1 = ResNet3D_Block(ch=num_classes, k_size=3)   \n",
    "\n",
    "        self.to_logits = nn.Conv3d(num_classes, num_classes, kernel_size=1)\n",
    "        self.reset_parameters()\n",
    "      \n",
    "    def reset_parameters(self):\n",
    "        for weight in self.parameters():\n",
    "            if weight.dim() > 1:\n",
    "                torch.nn.init.kaiming_uniform_(weight)\n",
    "\n",
    "    def forward(self, z_mean, z_log_sigma):\n",
    "        epsilon = torch.randn_like(z_log_sigma, device=self.device)\n",
    "        z = z_mean + z_log_sigma.exp() * epsilon\n",
    "        \n",
    "        x = self.linear_up(z)\n",
    "        x = self.relu(x)\n",
    "        x = x.view(-1, 64, 8, 8, 1)  \n",
    "\n",
    "        x = self.upsample4(x) \n",
    "        x = self.res_block4(x)\n",
    "\n",
    "        x = self.upsample3(x) \n",
    "        x = self.res_block3(x)\n",
    "\n",
    "        x = self.upsample2(x) \n",
    "        x = self.res_block2(x)\n",
    "\n",
    "        x = self.upsample1(x) \n",
    "        x = self.res_block1(x)\n",
    "\n",
    "        #print(x.shape)\n",
    "        logits = self.to_logits(x)\n",
    "        return logits\n",
    "\n",
    "\n",
    "class VAE3D(nn.Module):\n",
    "    def __init__(self, latent_dim=128, num_classes=6, device='cpu'):\n",
    "        super(VAE3D, self).__init__()\n",
    "        self.device = device\n",
    "        self.encoder = Encoder(latent_dim)\n",
    "        self.decoder = Decoder(latent_dim, num_classes, device)\n",
    "\n",
    "        self.cross_entropy = nn.CrossEntropyLoss()\n",
    "        self.reset_parameters()\n",
    "      \n",
    "    def reset_parameters(self):\n",
    "        for weight in self.parameters():\n",
    "            if weight.dim() > 1:\n",
    "                torch.nn.init.kaiming_uniform_(weight)\n",
    "\n",
    "    def forward(self, x):\n",
    "        z_mean, z_log_sigma = self.encoder(x)\n",
    "        y = self.decoder(z_mean, z_log_sigma)\n",
    "        return y, z_mean, z_log_sigma\n",
    "\n",
    "    def gaussian_kl_divergence(self, z_mean, z_log_sigma):\n",
    "        z_log_var = z_log_sigma * 2\n",
    "        kl_div = -0.5 * torch.sum(1 + z_log_var - z_mean.pow(2) - z_log_var.exp(), dim=1)\n",
    "        return torch.mean(kl_div)\n",
    "\n",
    "    def step(self, x, mask=None):\n",
    "        target = x\n",
    "        y, z_mean, z_log_sigma = self(x.unsqueeze(1).float()) #unsqueeze to add channel dim\n",
    "        kl_loss = self.gaussian_kl_divergence(z_mean, z_log_sigma)\n",
    "        ce_loss = self.cross_entropy(y, target)\n",
    "        #print(kl_loss, ce_loss)\n",
    "        return kl_loss * 0.001 + ce_loss\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "437463b9-1e77-4432-b429-238ed29a1877",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vae3d = VAE3D(latent_dim=512, num_classes=len(train_set.class_names))\n",
    "# from torchsummary import summary\n",
    "# summary(vae3d, input_size=(1, 128, 128, 16), device=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c6a552f6-cbf1-49e5-8906-5946e9160858",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = torch.rand(1, 1, 256, 256, 32)\n",
    "# y, z_mean, z_log_sigma = vae3d(x)\n",
    "# print(y.shape)\n",
    "# print(z_mean.shape)\n",
    "# print(z_log_sigma.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5585de6e-4765-426b-8cdb-bb23778d2f13",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a52e486c-9d4f-4708-88c5-4cd8552defba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd4f1760-bde3-409d-aee1-0f3411cf8c2e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d12ad5a8-6391-453e-a1ba-b426620f6752",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "332fc6fa-be80-48b2-a092-aa06c38a643a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "532918bb-7d43-41ae-af7e-cbe1d2a14e91",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f14bdc96-1296-46a9-85a7-300ef4bb5bda",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "80b9e1ba-d7a3-4659-9f2e-6651d5fe2bcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader, random_split\n",
    "num_classes = len(train_set.class_names)\n",
    "\n",
    "\n",
    "\n",
    "def rsplit(set, perc=0.7):\n",
    "    split_idx = int(perc * len(set))\n",
    "    return random_split(set, [split_idx, len(set) - split_idx])\n",
    "\n",
    "train_set_small, _ = rsplit(train_set, 0.01)\n",
    "train_subset, val_subset = rsplit(train_set_small, 0.85)\n",
    "\n",
    "train_dataloader = DataLoader(train_subset, batch_size=8, shuffle=True)\n",
    "val_dataloader = DataLoader(val_subset, batch_size=8, shuffle=True)\n",
    "\n",
    "\n",
    "\n",
    "print(len(train_dataloader))\n",
    "print(len(val_dataloader))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "eb80cc46-50f0-4fa3-aaf1-cd2e9f90d2e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VAE3D(latent_dim=512, num_classes=len(train_set.class_names), device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e1d8149-7c53-4f0b-a400-3e8779eabd2d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "599610d7-8b12-47c8-a501-8044345133b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Adam\n",
    "optimizer = Adam(model.parameters(), lr=0.001)\n",
    "num_epochs = 20\n",
    "best_loss = np.inf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4810051c-d6b0-4953-9c51-0f4bc0441c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0bd71d67-30f7-4837-87e7-8ec93402045c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('vae3d.pth', map_location=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "190c74fc-d3c4-40c7-8449-416aa63a5abb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a29ffc22-7703-4f56-a42f-86cc2b75f427",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 1/11 [00:05<00:59,  5.94s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[1;32m      7\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[0;32m----> 8\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, (left_img, right_img, voxel_labels, extra_labels) \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28menumerate\u001b[39m(train_dataloader),\n\u001b[1;32m      9\u001b[0m                                                                      total \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(train_dataloader)):\n\u001b[1;32m     10\u001b[0m         voxel_lidar, voxel_occluded, voxel_invalid  \u001b[38;5;241m=\u001b[39m extra_labels\n\u001b[1;32m     11\u001b[0m         voxel_labels \u001b[38;5;241m=\u001b[39m voxel_labels\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tqdm/std.py:1178\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1175\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1177\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1178\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m   1179\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m   1180\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1181\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:633\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    630\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    631\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    632\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 633\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    634\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    635\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    636\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    637\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:677\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    675\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    676\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 677\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    678\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    679\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataset.py:298\u001b[0m, in \u001b[0;36mSubset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    296\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(idx, \u001b[38;5;28mlist\u001b[39m):\n\u001b[1;32m    297\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m idx]]\n\u001b[0;32m--> 298\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataset.py:298\u001b[0m, in \u001b[0;36mSubset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    296\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(idx, \u001b[38;5;28mlist\u001b[39m):\n\u001b[1;32m    297\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m idx]]\n\u001b[0;32m--> 298\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\n",
      "Cell \u001b[0;32mIn[3], line 135\u001b[0m, in \u001b[0;36mKittiLoader.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;66;03m#  0 invalid 1-19 class and 255 invalid\u001b[39;00m\n\u001b[1;32m    134\u001b[0m voxel_labels \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mfromfile(label_name, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39muint16)\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m256\u001b[39m, \u001b[38;5;241m256\u001b[39m, \u001b[38;5;241m32\u001b[39m)\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[0;32m--> 135\u001b[0m voxel_labels \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvectorize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclass_map\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvoxel_labels\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    136\u001b[0m \u001b[38;5;66;03m#print(voxel_data.shape)\u001b[39;00m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;66;03m#print(voxel_labels.shape)\u001b[39;00m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_vox_occluded:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/numpy/lib/function_base.py:2329\u001b[0m, in \u001b[0;36mvectorize.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2326\u001b[0m     vargs \u001b[38;5;241m=\u001b[39m [args[_i] \u001b[38;5;28;01mfor\u001b[39;00m _i \u001b[38;5;129;01min\u001b[39;00m inds]\n\u001b[1;32m   2327\u001b[0m     vargs\u001b[38;5;241m.\u001b[39mextend([kwargs[_n] \u001b[38;5;28;01mfor\u001b[39;00m _n \u001b[38;5;129;01min\u001b[39;00m names])\n\u001b[0;32m-> 2329\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_vectorize_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/numpy/lib/function_base.py:2410\u001b[0m, in \u001b[0;36mvectorize._vectorize_call\u001b[0;34m(self, func, args)\u001b[0m\n\u001b[1;32m   2407\u001b[0m ufunc, otypes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_ufunc_and_otypes(func\u001b[38;5;241m=\u001b[39mfunc, args\u001b[38;5;241m=\u001b[39margs)\n\u001b[1;32m   2409\u001b[0m \u001b[38;5;66;03m# Convert args to object arrays first\u001b[39;00m\n\u001b[0;32m-> 2410\u001b[0m inputs \u001b[38;5;241m=\u001b[39m [asanyarray(a, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mobject\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m args]\n\u001b[1;32m   2412\u001b[0m outputs \u001b[38;5;241m=\u001b[39m ufunc(\u001b[38;5;241m*\u001b[39minputs)\n\u001b[1;32m   2414\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ufunc\u001b[38;5;241m.\u001b[39mnout \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/numpy/lib/function_base.py:2410\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   2407\u001b[0m ufunc, otypes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_ufunc_and_otypes(func\u001b[38;5;241m=\u001b[39mfunc, args\u001b[38;5;241m=\u001b[39margs)\n\u001b[1;32m   2409\u001b[0m \u001b[38;5;66;03m# Convert args to object arrays first\u001b[39;00m\n\u001b[0;32m-> 2410\u001b[0m inputs \u001b[38;5;241m=\u001b[39m [\u001b[43masanyarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mobject\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m args]\n\u001b[1;32m   2412\u001b[0m outputs \u001b[38;5;241m=\u001b[39m ufunc(\u001b[38;5;241m*\u001b[39minputs)\n\u001b[1;32m   2414\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ufunc\u001b[38;5;241m.\u001b[39mnout \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = 0.0\n",
    "    for i, (left_img, right_img, voxel_labels, extra_labels) in tqdm(enumerate(train_dataloader),\n",
    "                                                                     total = len(train_dataloader)):\n",
    "        voxel_lidar, voxel_occluded, voxel_invalid  = extra_labels\n",
    "        voxel_labels = voxel_labels.to(device)\n",
    "        voxel_invalid = voxel_invalid.to(device)\n",
    "        voxel_pgt = preprocess_gt(voxel_labels, voxel_invalid, (vf_mask).to(device))\n",
    "        loss = model.step(voxel_pgt)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=20.0)\n",
    "        \n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "        #print(loss.item())  \n",
    "    valid_loss = 0.0\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for (left_img, right_img, voxel_labels, extra_labels) in tqdm(val_dataloader):\n",
    "            voxel_lidar, voxel_occluded, voxel_invalid  = extra_labels\n",
    "            voxel_labels = voxel_labels.to(device)\n",
    "            voxel_invalid = voxel_invalid.to(device)\n",
    "            voxel_pgt = preprocess_gt(voxel_labels, voxel_invalid, (vf_mask).to(device))\n",
    "            loss = model.step(voxel_pgt)\n",
    "            \n",
    "            loss = model.step(voxel_pgt)\n",
    "            optimizer.zero_grad()\n",
    "            loss.requires_grad_(True)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            valid_loss += loss.item()\n",
    "            \n",
    "    print(f'Epoch {epoch + 1}: Training loss: {train_loss / len(train_dataloader)}, Validation loss: {valid_loss / len(val_dataloader)}')\n",
    "    if (train_loss / len(train_dataloader)) < best_loss:\n",
    "        torch.save(model.state_dict(), 'vae3d.pth')\n",
    "        best_loss = (train_loss / len(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89862496-5987-4f7d-9046-20226906aafc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c9afd51-6e15-4ed3-9598-94c5b0db2ec7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f464fafa-e8aa-45d6-a920-2f44fc3d19b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4815900-df1e-4526-a36e-ccb274d44720",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f88f9c-d19d-4aa7-bdd6-871f51efff37",
   "metadata": {},
   "outputs": [],
   "source": [
    "left_img, right_img, voxel_labels, extra_labels = train_subset.__getitem__(random.randint(0, len(train_subset)))\n",
    "voxel_lidar, voxel_occluded, voxel_invalid  = extra_labels\n",
    "voxel_labels = voxel_labels.unsqueeze(0).to(device)\n",
    "voxel_invalid = voxel_invalid.unsqueeze(0).to(device)\n",
    "voxel_pgt = preprocess_gt(voxel_labels, voxel_invalid, (vf_mask).to(device))\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    y, z_mean, z_log_sigma = model(voxel_pgt.unsqueeze(1).float())\n",
    "\n",
    "\n",
    "plot_tensor2d(left_img)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3246f163-a521-4f1a-b08a-62cabf284063",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = torch.argmax(y, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cb01cc3-de6f-4776-a8d0-6a3604e34e46",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c523887-1a07-4bc5-916c-00deda8c8061",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_labeled_array3d((pred.view(128, 128, 16).float()).detach().cpu().numpy(), size = 1, marker = 'box')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a39772ba-16d5-4f97-8efa-83d8d5b8bbeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_labeled_array3d((voxel_pgt.view(128, 128, 16)).detach().cpu().numpy(), size = 1, marker = 'box')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d811ec7-2cf2-4e8d-8ba3-cf5e22aa10ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0afe02d0-b027-4cca-85e1-6d4ee2b716ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "003c5aab-a3b4-4659-9456-7ba6ac51ba89",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b94f537-c768-473b-a78b-53e328289501",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f3f7f21-822a-4028-9c19-7d6aca160b58",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9addeeba-52f9-43c0-b9cd-609d46aa94ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b1e8f4a-81b2-4942-a9c2-24ba57d0f3ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fcfaf42-ab1a-436e-aa59-0a2f7c27a96c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dea8b5d-09d7-4155-b5c6-8a9706294359",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7523b48a-0cfb-4dca-a2da-778cc872eb8c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42012e5e-0339-4a4b-97ad-8f9a334b6e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from typing import List, Callable, Union, Any, TypeVar, Tuple\n",
    "Tensor = TypeVar('torch.tensor')\n",
    "from abc import abstractmethod\n",
    "\n",
    "class BaseVAE(nn.Module):\n",
    "    \n",
    "    def __init__(self) -> None:\n",
    "        super(BaseVAE, self).__init__()\n",
    "\n",
    "    def encode(self, input: Tensor) -> List[Tensor]:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def decode(self, input: Tensor) -> Any:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def sample(self, batch_size:int, current_device: int, **kwargs) -> Tensor:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def generate(self, x: Tensor, **kwargs) -> Tensor:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    @abstractmethod\n",
    "    def forward(self, *inputs: Tensor) -> Tensor:\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def loss_function(self, *inputs: Any, **kwargs) -> Tensor:\n",
    "        pass\n",
    "\n",
    "\n",
    "class CategoricalVAE(BaseVAE):\n",
    "\n",
    "    def __init__(self,\n",
    "                 in_channels: int,\n",
    "                 latent_dim: int,\n",
    "                 categorical_dim: int = 40, # Num classes\n",
    "                 hidden_dims: List = None,\n",
    "                 temperature: float = 0.5,\n",
    "                 anneal_rate: float = 3e-5,\n",
    "                 anneal_interval: int = 100, # every 100 batches\n",
    "                 alpha: float = 30.,\n",
    "                 **kwargs) -> None:\n",
    "        super(CategoricalVAE, self).__init__()\n",
    "\n",
    "        self.latent_dim = latent_dim\n",
    "        self.categorical_dim = categorical_dim\n",
    "        self.temp = temperature\n",
    "        self.min_temp = temperature\n",
    "        self.anneal_rate = anneal_rate\n",
    "        self.anneal_interval = anneal_interval\n",
    "        self.alpha = alpha\n",
    "\n",
    "        modules = []\n",
    "        if hidden_dims is None:\n",
    "            hidden_dims = [32, 64, 128, 256, 512]\n",
    "\n",
    "        # Build Encoder\n",
    "        for h_dim in hidden_dims:\n",
    "            modules.append(\n",
    "                nn.Sequential(\n",
    "                    nn.Conv2d(in_channels, out_channels=h_dim,\n",
    "                              kernel_size= 3, stride= 2, padding  = 1),\n",
    "                    nn.BatchNorm2d(h_dim),\n",
    "                    nn.LeakyReLU())\n",
    "            )\n",
    "            in_channels = h_dim\n",
    "\n",
    "        self.encoder = nn.Sequential(*modules)\n",
    "        self.fc_z = nn.Linear(hidden_dims[-1]*4,\n",
    "                               self.latent_dim * self.categorical_dim)\n",
    "\n",
    "        # Build Decoder\n",
    "        modules = []\n",
    "\n",
    "        self.decoder_input = nn.Linear(self.latent_dim * self.categorical_dim\n",
    "                                       , hidden_dims[-1] * 4)\n",
    "\n",
    "        hidden_dims.reverse()\n",
    "\n",
    "        for i in range(len(hidden_dims) - 1):\n",
    "            modules.append(\n",
    "                nn.Sequential(\n",
    "                    nn.ConvTranspose2d(hidden_dims[i],\n",
    "                                       hidden_dims[i + 1],\n",
    "                                       kernel_size=3,\n",
    "                                       stride = 2,\n",
    "                                       padding=1,\n",
    "                                       output_padding=1),\n",
    "                    nn.BatchNorm2d(hidden_dims[i + 1]),\n",
    "                    nn.LeakyReLU())\n",
    "            )\n",
    "\n",
    "\n",
    "\n",
    "        self.decoder = nn.Sequential(*modules)\n",
    "\n",
    "        self.final_layer = nn.Sequential(\n",
    "                            nn.ConvTranspose2d(hidden_dims[-1],\n",
    "                                               hidden_dims[-1],\n",
    "                                               kernel_size=3,\n",
    "                                               stride=2,\n",
    "                                               padding=1,\n",
    "                                               output_padding=1),\n",
    "                            nn.BatchNorm2d(hidden_dims[-1]),\n",
    "                            nn.LeakyReLU(),\n",
    "                            nn.Conv2d(hidden_dims[-1], out_channels= 3,\n",
    "                                      kernel_size= 3, padding= 1),\n",
    "                            nn.Tanh())\n",
    "        self.sampling_dist = torch.distributions.OneHotCategorical(1. / categorical_dim * torch.ones((self.categorical_dim, 1)))\n",
    "\n",
    "    def encode(self, input: Tensor) -> List[Tensor]:\n",
    "        \"\"\"\n",
    "        Encodes the input by passing through the encoder network\n",
    "        and returns the latent codes.\n",
    "        :param input: (Tensor) Input tensor to encoder [B x C x H x W]\n",
    "        :return: (Tensor) Latent code [B x D x Q]\n",
    "        \"\"\"\n",
    "        result = self.encoder(input)\n",
    "        result = torch.flatten(result, start_dim=1)\n",
    "\n",
    "        # Split the result into mu and var components\n",
    "        # of the latent Gaussian distribution\n",
    "        z = self.fc_z(result)\n",
    "        z = z.view(-1, self.latent_dim, self.categorical_dim)\n",
    "        return [z]\n",
    "\n",
    "    def decode(self, z: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Maps the given latent codes\n",
    "        onto the image space.\n",
    "        :param z: (Tensor) [B x D x Q]\n",
    "        :return: (Tensor) [B x C x H x W]\n",
    "        \"\"\"\n",
    "        result = self.decoder_input(z)\n",
    "        result = result.view(-1, 512, 2, 2)\n",
    "        result = self.decoder(result)\n",
    "        result = self.final_layer(result)\n",
    "        return result\n",
    "\n",
    "    def reparameterize(self, z: Tensor, eps:float = 1e-7) -> Tensor:\n",
    "        \"\"\"\n",
    "        Gumbel-softmax trick to sample from Categorical Distribution\n",
    "        :param z: (Tensor) Latent Codes [B x D x Q]\n",
    "        :return: (Tensor) [B x D]\n",
    "        \"\"\"\n",
    "        # Sample from Gumbel\n",
    "        u = torch.rand_like(z)\n",
    "        g = - torch.log(- torch.log(u + eps) + eps)\n",
    "\n",
    "        # Gumbel-Softmax sample\n",
    "        s = F.softmax((z + g) / self.temp, dim=-1)\n",
    "        s = s.view(-1, self.latent_dim * self.categorical_dim)\n",
    "        return s\n",
    "\n",
    "\n",
    "    def forward(self, input: Tensor, **kwargs) -> List[Tensor]:\n",
    "        q = self.encode(input)[0]\n",
    "        z = self.reparameterize(q)\n",
    "        return  [self.decode(z), input, q]\n",
    "\n",
    "    def loss_function(self,\n",
    "                      *args,\n",
    "                      **kwargs) -> dict:\n",
    "        \"\"\"\n",
    "        Computes the VAE loss function.\n",
    "        KL(N(\\mu, \\sigma), N(0, 1)) = \\log \\frac{1}{\\sigma} + \\frac{\\sigma^2 + \\mu^2}{2} - \\frac{1}{2}\n",
    "        :param args:\n",
    "        :param kwargs:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        recons = args[0]\n",
    "        input = args[1]\n",
    "        q = args[2]\n",
    "\n",
    "        q_p = F.softmax(q, dim=-1) # Convert the categorical codes into probabilities\n",
    "\n",
    "        kld_weight = kwargs['M_N'] # Account for the minibatch samples from the dataset\n",
    "        batch_idx = kwargs['batch_idx']\n",
    "\n",
    "        # Anneal the temperature at regular intervals\n",
    "        if batch_idx % self.anneal_interval == 0 and self.training:\n",
    "            self.temp = np.maximum(self.temp * np.exp(- self.anneal_rate * batch_idx),\n",
    "                                   self.min_temp)\n",
    "\n",
    "        recons_loss =F.mse_loss(recons, input, reduction='mean')\n",
    "\n",
    "        # KL divergence between gumbel-softmax distribution\n",
    "        eps = 1e-7\n",
    "\n",
    "        # Entropy of the logits\n",
    "        h1 = q_p * torch.log(q_p + eps)\n",
    "\n",
    "        # Cross entropy with the categorical distribution\n",
    "        h2 = q_p * np.log(1. / self.categorical_dim + eps)\n",
    "        kld_loss = torch.mean(torch.sum(h1 - h2, dim =(1,2)), dim=0)\n",
    "\n",
    "        # kld_weight = 1.2\n",
    "        loss = self.alpha * recons_loss + kld_weight * kld_loss\n",
    "        return {'loss': loss, 'Reconstruction_Loss':recons_loss, 'KLD':-kld_loss}\n",
    "\n",
    "    def sample(self,\n",
    "               num_samples:int,\n",
    "               current_device: int, **kwargs) -> Tensor:\n",
    "        \"\"\"\n",
    "        Samples from the latent space and return the corresponding\n",
    "        image space map.\n",
    "        :param num_samples: (Int) Number of samples\n",
    "        :param current_device: (Int) Device to run the model\n",
    "        :return: (Tensor)\n",
    "        \"\"\"\n",
    "        # [S x D x Q]\n",
    "\n",
    "        M = num_samples * self.latent_dim\n",
    "        np_y = np.zeros((M, self.categorical_dim), dtype=np.float32)\n",
    "        np_y[range(M), np.random.choice(self.categorical_dim, M)] = 1\n",
    "        np_y = np.reshape(np_y, [M // self.latent_dim, self.latent_dim, self.categorical_dim])\n",
    "        z = torch.from_numpy(np_y)\n",
    "\n",
    "        # z = self.sampling_dist.sample((num_samples * self.latent_dim, ))\n",
    "        z = z.view(num_samples, self.latent_dim * self.categorical_dim).to(current_device)\n",
    "        samples = self.decode(z)\n",
    "        return samples\n",
    "\n",
    "    def generate(self, x: Tensor, **kwargs) -> Tensor:\n",
    "        \"\"\"\n",
    "        Given an input image x, returns the reconstructed image\n",
    "        :param x: (Tensor) [B x C x H x W]\n",
    "        :return: (Tensor) [B x C x H x W]\n",
    "        \"\"\"\n",
    "\n",
    "        return self.forward(x)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78ae19fa-dc0e-42b0-95e2-531f52bb01af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da68f9e3-d6fd-4b34-9fc2-8f20c1dba88c",
   "metadata": {},
   "outputs": [],
   "source": [
    "vae = CategoricalVAE(in_channels = 8, latent_dim = 256,\n",
    "                 categorical_dim = len(train_set.class_names), # Num classes\n",
    "                 hidden_dims = [32, 64, 128, 256, 512])\n",
    "\n",
    "from torchsummary import summary\n",
    "summary(vae, input_size=(8, 64, 64), device=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df52c0b4-8e5a-4b38-ab56-b737fe72e417",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "513c0739-588c-408e-ab0a-6487768a7b37",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e44b81-3cda-49f6-8a00-bcb9b45d2b6f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b3ccad6-05f3-4530-94cd-b04242e03d8c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e426d2b-51ab-4f4c-b3a1-df08d289bf05",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d1acb15-8fcf-4057-8a01-aa8b61e91cfa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c802b1c4-6cad-4f41-a2d8-29146f1c41c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from monoscene.monoscene import MonoScene\n",
    "from utils.monoscene_utils import *\n",
    "\n",
    "pretrained_weight_path = '/workspace/PretrainedWeights/monoscene_kitti.ckpt'\n",
    "\n",
    "\n",
    "monoscene_pt = MonoScene.load_from_checkpoint(\n",
    "        pretrained_weight_path,\n",
    "        dataset=\"kitti\",\n",
    "        n_classes=20,\n",
    "        feature = 64,\n",
    "        project_scale = 2,\n",
    "        full_scene_size = (256, 256, 32),\n",
    ")\n",
    "\n",
    "def build_monoscene_input(img):\n",
    "    img = img.unsqueeze(0)\n",
    "    batch_dict = get_projections(img_width, img_height, calib)\n",
    "    for key in batch_dict:\n",
    "        batch_dict[key] = batch_dict[key].unsqueeze(0)\n",
    "        batch_dict[key] = batch_dict[key].to(device)\n",
    "    batch_dict[\"img\"] = img.to(device)\n",
    "    return batch_dict\n",
    "\n",
    "for p in monoscene_pt.parameters():\n",
    "     p.requires_grad = False\n",
    "\n",
    "\n",
    "\n",
    "class MonoScene(nn.Module):\n",
    "    def __init__(self, MonoScene_pretrained, calib, img_width, img_height):\n",
    "        super(MonoScene, self).__init__()\n",
    "\n",
    "        self.monoscene_pt = MonoScene_pretrained\n",
    "        self.batch_dict = get_projections(img_width, img_height, calib)\n",
    "        for key in self.batch_dict:\n",
    "            self.batch_dict[key] = self.batch_dict[key].unsqueeze(0)\n",
    "            self.batch_dict[key] = self.batch_dict[key].to(device)\n",
    "        \n",
    "    \n",
    "    # input_tensor 'x' should be batched image tensor with\n",
    "    # shape: N x C x H x W, where N is the batch size,  \n",
    "    # and C,H,W are the dimensions of RGB images \n",
    "    def forward(self, x):\n",
    "        #print(x.shape)\n",
    "\n",
    "        self.batch_dict[\"img\"] = x.to(device)\n",
    "\n",
    "        x = self.monoscene_pt(self.batch_dict)\n",
    "        return x\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "print(\"Number of parameters (in millions):\", sum(p.numel() for p in monoscene_pt.parameters()) / 1_000_000, 'M')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
