{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ebcf2ec3-c8c1-4871-8c6e-5f49cba9f710",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import argparse\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import random\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchsummary import summary\n",
    "\n",
    "import bqplot.scales\n",
    "import ipyvolume as ipv\n",
    "import ipywidgets as widgets\n",
    "\n",
    "\n",
    "torch.cuda.is_available()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52bb5216-a8bf-479e-8f10-6b1d6aef4f4d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1765bb9c-f001-435a-84d3-5f756b759a9d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1362\n"
     ]
    }
   ],
   "source": [
    "sys.path.append(\"/workspace/HKU-OccNet/\")\n",
    "from utils import SemanticKITTIDataset\n",
    "from utils import visualize_labeled_array3d\n",
    "from utils import plot_tensor2d\n",
    "\n",
    "KITTI_DIR = \"/workspace/Dataset/dataset\"\n",
    "\n",
    "\n",
    "\n",
    "train_set = SemanticKITTIDataset(root_dir=KITTI_DIR, mode='train', \n",
    "                                 sequences=['00'], split_ratio=0.3)\n",
    "print(len(train_set))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d89cfe-64da-4d48-bf8a-b0836deb859c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b22b6967-27fc-4755-af1a-887d78113c5a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#left_img, right_img, vox_labels = train_set.get_data(666)\n",
    "# plt.figure(1)\n",
    "# plot_tensor2d(left_img)\n",
    "# plt.figure(2)\n",
    "# plot_tensor2d(right_img)\n",
    "# plt.show()\n",
    "#visualize_labeled_array3d(vox_labels.numpy().astype(np.uint16), size = 0.5, marker = 'box')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0feca197-92bb-4412-acd6-ed6f6933c462",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#left_img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e2d19b9-3544-4bd5-8cd9-c84ddb00b1fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0ae7b44d-c900-4e07-ad82-549202b5f4c0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_set, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a89108ca-bc63-426c-9a29-c14e6fb33718",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_classes = len(train_set.class_names)\n",
    "class_weights = train_set.class_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96934b4a-42d1-4f23-99a4-67a8f05dcb55",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc837fc5-5391-402d-9bf5-7887550d9808",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c3f1929-ebbe-4afc-b7a0-348cd78f9df6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9827529-6450-4ef0-8317-a7ff13a79ab1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d0d5e89-c81e-47ee-aa8f-5bfe2ac64f19",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d284f43d-ce2c-49f7-b123-3cb80344ab5d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") \n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "521484c5-1fb6-45aa-bcde-01563e806ec5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ae98db0e-9573-4074-8bc7-13d24f220c63",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "img_width, img_height = 1241, 376"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2ba6b047-3ca8-4e40-8d82-70f3cc161d91",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/pytorch_lightning/utilities/migration/migration.py:200: PossibleUserWarning: You have multiple `ModelCheckpoint` callback states in this checkpoint, but we found state keys that would end up colliding with each other after an upgrade, which means we can't differentiate which of your checkpoint callbacks needs which states. At least one of your `ModelCheckpoint` callbacks will not be able to reload the state.\n",
      "  category=PossibleUserWarning,\n",
      "Lightning automatically upgraded your loaded checkpoint from v1.1.3 to v1.9.5. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint --file ../../PretrainedWeights/monoscene_kitti.ckpt`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_relations 4\n",
      "Loading base model ()..."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/rwightman_gen-efficientnet-pytorch_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n",
      "Removing last two layers (global_pool & classifier).\n",
      "Building Encoder-Decoder model..Done.\n",
      "Monoscene parameter: 149555444\n"
     ]
    }
   ],
   "source": [
    "from monoscene.monoscene import MonoScene\n",
    "from utils.monoscene_utils import *\n",
    "\n",
    "pretrained_weight_path = '/workspace/PretrainedWeights/monoscene_kitti.ckpt'\n",
    "\n",
    "\n",
    "monoscene_pt = MonoScene.load_from_checkpoint(\n",
    "        pretrained_weight_path,\n",
    "        dataset=\"kitti\",\n",
    "        n_classes=20,\n",
    "        feature = 64,\n",
    "        project_scale = 2,\n",
    "        full_scene_size = (256, 256, 32),\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class MonoScene(nn.Module):\n",
    "    def __init__(self, MonoScene_pretrained, calib, img_width, img_height):\n",
    "        super(MonoScene, self).__init__()\n",
    "\n",
    "        self.monoscene_pt = MonoScene_pretrained\n",
    "        self.batch_dict = get_projections(img_width, img_height, calib)\n",
    "        for key in self.batch_dict:\n",
    "            self.batch_dict[key] = self.batch_dict[key].unsqueeze(0)\n",
    "            self.batch_dict[key] = self.batch_dict[key].to(device)\n",
    "        \n",
    "    \n",
    "    # input_tensor 'x' should be batched stereo image tensor with\n",
    "    # shape: N x 2 x C x H x W, where N is the batch size, \n",
    "    # 2 for left and right images, \n",
    "    # and C,H,W are the dimensions of RGB images \n",
    "    def forward(self, x):\n",
    "        #print(x.shape)\n",
    "\n",
    "        self.batch_dict[\"img\"] = x.to(device)\n",
    "\n",
    "        x = self.monoscene_pt(self.batch_dict)\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "print(\"Monoscene parameter:\", sum(p.numel() for p in monoscene_pt.parameters() if p.requires_grad))\n",
    "for p in monoscene_pt.parameters():\n",
    "     p.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3aea582f-29fe-4883-bde4-acc3cfb71d68",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "calib = read_calib(\"/workspace/HKU-OccNet/calib.txt\")\n",
    "\n",
    "#get_projections(img_width, img_height, calib)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1038f247-7974-4204-ab40-0ee5cf9674b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d75e1040-c9f8-4657-98f5-390b89202a69",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b62a061e-f244-4c2c-8892-049c04c581bb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-trained model successfully loaded.\n",
      "Number of parameters (in millions): 2.513811 M\n"
     ]
    }
   ],
   "source": [
    "from utils import load_STTR_model\n",
    "from utils import NestedTensor, batched_index_select\n",
    "\n",
    "\n",
    "pretrained_weight_path = '/workspace/PretrainedWeights/kitti_finetuned_model.pth.tar'\n",
    "\n",
    "sttr_pt = load_STTR_model(pretrained_weight_path)\n",
    "for param in sttr_pt.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "\n",
    "print(\"Number of parameters (in millions):\", sum(p.numel() for p in sttr_pt.parameters()) / 1_000_000, 'M')\n",
    "\n",
    "\n",
    "class STTR_InputAdapterLayer(nn.Module):\n",
    "    def __init__(self, downsample=3):\n",
    "        super(STTR_InputAdapterLayer, self).__init__()\n",
    "        self.downsample = downsample\n",
    "\n",
    "    def forward(self, input_tensor):\n",
    "        input_tensor = input_tensor\n",
    "        bs, _, _, h, w = input_tensor.shape  # Extract batch size, height, and width\n",
    "\n",
    "        # Extract left and right images from the input tensor\n",
    "        left_imgs = input_tensor[:, 0, :, :, :].squeeze(1)\n",
    "        right_imgs = input_tensor[:, 1, :, :, :].squeeze(1)\n",
    "\n",
    "        col_offset = int(self.downsample / 2)\n",
    "        row_offset = int(self.downsample / 2)\n",
    "        sampled_cols = torch.arange(col_offset, w, self.downsample)[None,].expand(bs, -1).cuda()\n",
    "        sampled_rows = torch.arange(row_offset, h, self.downsample)[None,].expand(bs, -1).cuda()\n",
    "        \n",
    "\n",
    "        # Create NestedTensor for the batch\n",
    "        nested_tensor = NestedTensor(left_imgs, right_imgs,  \n",
    "                                    sampled_cols=sampled_cols, sampled_rows=sampled_rows)\n",
    "\n",
    "        return nested_tensor\n",
    "\n",
    "class STTR(nn.Module):\n",
    "    def __init__(self, STTR_pretrained):\n",
    "        super(STTR, self).__init__()\n",
    "        self.sttr_adapter_layer = STTR_InputAdapterLayer(downsample=3)\n",
    "        self.sttr_pt = STTR_pretrained\n",
    "        \n",
    "    \n",
    "    # input_tensor 'x' should be batched stereo image tensor with\n",
    "    # shape: N x 2 x C x H x W, where N is the batch size, \n",
    "    # 2 for left and right images, \n",
    "    # and C,H,W are the dimensions of RGB images \n",
    "    def forward(self, x): \n",
    "        x = self.sttr_adapter_layer(x)\n",
    "        # bs, _, h, w = x.left.size()\n",
    "        # feat = self.sttr_pt.backbone(x)\n",
    "        # tokens = self.sttr_pt.tokenizer(feat)\n",
    "        # pos_enc = self.sttr_pt.pos_encoder(x)\n",
    "        # # separate left and right\n",
    "        # feat_left = tokens[:bs]\n",
    "        # feat_right = tokens[bs:]  # NxCxHxW\n",
    "        # # downsample\n",
    "        # if x.sampled_cols is not None:\n",
    "        #     feat_left = batched_index_select(feat_left, 3, x.sampled_cols)\n",
    "        #     feat_right = batched_index_select(feat_right, 3, x.sampled_cols)\n",
    "        # if x.sampled_rows is not None:\n",
    "        #     feat_left = batched_index_select(feat_left, 2, x.sampled_rows)\n",
    "        #     feat_right = batched_index_select(feat_right, 2, x.sampled_rows)\n",
    "        # attn_weight = self.sttr_pt.transformer(feat_left, feat_right, pos_enc)\n",
    "        # output = self.sttr_pt.regression_head(attn_weight, x)\n",
    "        output = self.sttr_pt(x)\n",
    "        disp_map = output['disp_pred'][0]\n",
    "        occ_map = output['occ_pred'][0] > 0.5\n",
    "        disp_map[occ_map] = 0.0\n",
    "        \n",
    "        return disp_map\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d29f60c3-008c-4b20-9bc1-47106f98e501",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8ee34085-59b7-4b49-bc03-dd9bc981d6e0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "\n",
    "# class StereoToVoxelNet(nn.Module):\n",
    "#     def __init__(self, input_channels=3, H=376, W=1241):\n",
    "#         super(StereoToVoxelNet, self).__init__()\n",
    "        \n",
    "#         self.conv1_stereo = nn.Conv2d(input_channels * 2, 32, kernel_size=3, stride=1, padding=1)\n",
    "#         self.conv2_stereo = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "#         self.conv3_stereo = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "#         # MLP layers for depth disparity map\n",
    "#         self.mlp1 = nn.Linear(H * W, 1024)\n",
    "#         self.mlp2 = nn.Linear(1024, 2048)\n",
    "\n",
    "#         self.adaptive_pool_stereo = nn.AdaptiveAvgPool2d((64, 64))\n",
    "\n",
    "#         # Upsample layer to upscale stereo features to match size before concatenation\n",
    "#         self.upsample = nn.Upsample(size=(128, 128), mode='bilinear', align_corners=False)\n",
    "\n",
    "#         # Final layers\n",
    "#         self.final_conv = nn.Conv2d(128 * 128 + 2048, 128, kernel_size=3, stride=1, padding=1)\n",
    "#         self.reshape = nn.Unflatten(1, (128, 128, 128, 16))\n",
    "\n",
    "#     def forward(self, input_tensor, depth_disp):\n",
    "#         N, _, C, H, W = input_tensor.shape\n",
    "#         input_tensor = input_tensor.view(N, -1, H, W)\n",
    "\n",
    "#         # Process stereo images\n",
    "#         x_stereo = nn.ReLU()(self.conv1_stereo(input_tensor))\n",
    "#         x_stereo = nn.ReLU()(self.conv2_stereo(x_stereo))\n",
    "#         x_stereo = nn.ReLU()(self.conv3_stereo(x_stereo))\n",
    "#         x_stereo = self.adaptive_pool_stereo(x_stereo)\n",
    "\n",
    "#         # Process depth disparity map\n",
    "#         #print(depth_disp.shape)\n",
    "#         depth_flat = depth_disp.view(N, -1)  # Flatten the depth map\n",
    "#         #print(depth_flat.shape)\n",
    "#         depth_features = nn.ReLU()(self.mlp1(depth_flat))\n",
    "#         depth_features = nn.ReLU()(self.mlp2(depth_features))\n",
    "\n",
    "#         # Combine features from stereo and depth\n",
    "#         x_combined = torch.cat([x_stereo.flatten(1), depth_features], dim=1)\n",
    "\n",
    "#         # Final processing\n",
    "#         print(x_combined.shape) #(2, 526336)\n",
    "#         x = nn.ReLU()(self.final_conv(x_combined.view(N, -1, 128, 128)))\n",
    "        \n",
    "#         x = self.reshape(x)\n",
    "\n",
    "#         return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d8e6948c-d95b-4622-87b5-6799ec40d509",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from utils import sem_scal_loss, geo_scal_loss, CE_ssc_loss\n",
    "from utils import Header\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3c9bae4c-99c4-47eb-939f-b31852793f45",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OMP: Info #276: omp_set_nested routine deprecated, please use omp_set_max_active_levels instead.\n"
     ]
    }
   ],
   "source": [
    "sttr = STTR(sttr_pt)\n",
    "monoscene = MonoScene(monoscene_pt, calib, img_width, img_height)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9b418fe8-bd9d-4897-b984-e023f4635911",
   "metadata": {},
   "outputs": [],
   "source": [
    "# img shape: 3, 376, 1241 \n",
    "# voxel shape: 256, 256, 32\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cfced9e-e4ac-4a70-bbd1-c43312bf8163",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b49ddc4d-1b2c-4b2e-882f-84f3fa27752d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5fba344-9f7d-41f9-bf89-a1e8be1a9fff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0bed33d1-86b3-42fb-9f6a-aceb64aad277",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ImageDepthEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ImageDepthEncoder, self).__init__()\n",
    "        # Image encoder (e.g., a simple CNN)\n",
    "        self.image_encoder = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "\n",
    "        # Depth map encoder (e.g., another CNN)\n",
    "        self.depth_encoder = nn.Sequential(\n",
    "            nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "\n",
    "    def forward(self, image, depth_map):\n",
    "        image_features = self.image_encoder(image)\n",
    "        depth_map = depth_map.unsqueeze(1)  # Add channel dimension\n",
    "        depth_features = self.depth_encoder(depth_map)\n",
    "        # Concatenate features along channel dimension\n",
    "        combined_features = torch.cat([image_features, depth_features], dim=1)\n",
    "        return combined_features\n",
    "\n",
    "class DepthProjectVoxel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DepthProjectVoxel, self).__init__()\n",
    "        self.encoder = ImageDepthEncoder()\n",
    "        # 3D Convolution layers for voxel projection\n",
    "        self.conv3d_layers = nn.Sequential(\n",
    "            nn.Conv3d(256, 128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv3d(128, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv3d(64, 1, kernel_size=3, stride=1, padding=1),\n",
    "            nn.Sigmoid()  # Assuming occupancy probabilities in [0, 1]\n",
    "        )\n",
    "\n",
    "    def forward(self, image, depth_map):\n",
    "        combined_features = self.encoder(image, depth_map)\n",
    "        #print(combined_features.shape) #printed torch.Size([1, 256, 94, 310])\n",
    "        \n",
    "        # Reshape and expand dimensions to fit 3D convolution\n",
    "        combined_features = combined_features.view(-1, 256, 1, 47, 155) \n",
    "        voxel_probabilities = self.conv3d_layers(combined_features)\n",
    "        print(voxel_probabilities.shape)\n",
    "        \n",
    "        voxel_probabilities = F.interpolate(voxel_probabilities, size=(256, 256, 32), mode='trilinear', align_corners=True)\n",
    "\n",
    "        voxel_probabilities = voxel_probabilities.squeeze(0).squeeze(0)  \n",
    "       \n",
    "        voxel_probabilities = nn.AdaptiveAvgPool3d((256, 256, 32))(voxel_probabilities)\n",
    "\n",
    "        return voxel_probabilities\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2e68dcaa-35dc-4a44-b758-66e50d0b9bbc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CrossAttentionModule(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(CrossAttentionModule, self).__init__()\n",
    "        # Define layers for cross-attention\n",
    "        self.query_conv = nn.Conv3d(in_channels, out_channels, kernel_size=1)\n",
    "        self.key_conv = nn.Conv3d(in_channels, out_channels, kernel_size=1)\n",
    "        self.value_conv = nn.Conv3d(in_channels, out_channels, kernel_size=1)\n",
    "\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, query, key, value):\n",
    "        batch_size = query.shape[0]\n",
    "\n",
    "        # Transform query, key, value\n",
    "        query = self.query_conv(query)\n",
    "        key = self.key_conv(key)\n",
    "        value = self.value_conv(value)\n",
    "\n",
    "        # Reshape for matmul\n",
    "        query = query.view(batch_size, -1, query.shape[2]*query.shape[3]*query.shape[4])\n",
    "        key = key.view(batch_size, -1, key.shape[2]*key.shape[3]*key.shape[4]).permute(0, 2, 1)\n",
    "        value = value.view(batch_size, -1, value.shape[2]*value.shape[3]*value.shape[4])\n",
    "\n",
    "        # Attention mechanism\n",
    "        attention = torch.bmm(query, key)\n",
    "        attention = self.softmax(attention)\n",
    "        out = torch.bmm(value, attention.permute(0, 2, 1))\n",
    "\n",
    "        # Reshape back to original size\n",
    "        out = out.view(batch_size, -1, query.shape[2], query.shape[3], query.shape[4])\n",
    "        return out\n",
    "\n",
    "class VoxelCrossAttn(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(VoxelCrossAttn, self).__init__()\n",
    "        self.monoscene = ...  # Your existing MonoScene model\n",
    "        self.cross_attention = CrossAttentionModule(num_classes, num_classes)\n",
    "        self.additional_layers = ...  # Additional layers as needed\n",
    "\n",
    "    def forward(self, pred_1h, vox_proposal):\n",
    "        mono_pred_1h = pred_1h\n",
    "\n",
    "        # Cross-attention\n",
    "        vox_proposal_expanded = vox_proposal.unsqueeze(0).expand_as(mono_pred_1h)\n",
    "        attn_output = self.cross_attention(vox_proposal_expanded, mono_pred_1h, mono_pred_1h)\n",
    "\n",
    "        # Additional layers for processing\n",
    "        output = self.additional_layers(attn_output)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7daa0763-fc0a-4ac2-839d-3e254b83352c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class STFB_Occ(nn.Module):\n",
    "    def __init__(self, num_classes, sttr, monoscene):\n",
    "        super(STFB_Occ, self).__init__()\n",
    "        self.sttr = sttr\n",
    "        self.monoscene = monoscene\n",
    "        \n",
    "        self.depth_prj_voxel = DepthProjectVoxel()\n",
    "        \n",
    "\n",
    "        # Initial convolution layers\n",
    "        self.conv1 = nn.Conv2d(4, 64, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
    "\n",
    "        # FPN Layers\n",
    "        self.toplayer = nn.Conv2d(256, 256, kernel_size=1, stride=1, padding=0)  # Reduce channels\n",
    "        self.latlayer1 = nn.Conv2d(128, 256, kernel_size=1, stride=1, padding=0)\n",
    "        self.latlayer2 = nn.Conv2d(64, 256, kernel_size=1, stride=1, padding=0)\n",
    "\n",
    "        self.smooth1 = nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1)\n",
    "        self.smooth2 = nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1)\n",
    "        \n",
    "        self.channel_reducer = nn.Conv2d(in_channels=768, out_channels=3, kernel_size=1)\n",
    "        \n",
    "        self.cross_attn = VoxelCrossAttn(num_classes)\n",
    "\n",
    "    def _upsample_add(self, x, y):\n",
    "        _, _, H, W = y.size()\n",
    "        return nn.functional.interpolate(x, size=(H, W), mode='bilinear', align_corners=False) + y\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Existing STTR depth estimation\n",
    "        x_depth = self.sttr(x)\n",
    "        x_depth = x_depth.unsqueeze(0)\n",
    "        \n",
    "\n",
    "        left_image = x[:, 0, :, :, :].squeeze(1)  # Shape: [N, C, H, W] (N always 1)\n",
    "        \n",
    "        print(left_image.shape, x_depth.shape)\n",
    "        vox_proposal = self.depth_prj_voxel(left_image, x_depth)\n",
    "        print(vox_proposal.shape)\n",
    "        \n",
    "        \n",
    "        x = torch.cat((left_image, x_depth.unsqueeze(0)), dim=1)  # Shape: [N, C+1, H, W]\n",
    "\n",
    "        # Convolution layers\n",
    "        c1 = self.conv1(x)\n",
    "        c2 = self.conv2(c1)\n",
    "        c3 = self.conv3(c2)\n",
    "\n",
    "        # Top-down pathway\n",
    "        p3 = self.toplayer(c3)\n",
    "        p2 = self._upsample_add(p3, self.latlayer1(c2))\n",
    "        p1 = self._upsample_add(p2, self.latlayer2(c1))\n",
    "\n",
    "        # Smoothing\n",
    "        p2 = self.smooth1(p2)\n",
    "        p1 = self.smooth2(p1)\n",
    "\n",
    "        # Final monoscene processing\n",
    "        fused_features = torch.cat([p1, p2, p3], dim=1)\n",
    "        reduced_features = self.channel_reducer(fused_features)\n",
    "        \n",
    "        #print(fused_features.shape)\n",
    "        mono_pred_1h = self.monoscene(reduced_features) \n",
    "        # mono_pred_1h has shape [batch_size, num_classes, 255, 255, 32]\n",
    "        #print(mono_pred_1h.shape) #[1, 20, 256, 256, 32]\n",
    "        # vox_propasal has shape = [255, 255, 32]\n",
    "        \n",
    "        vox_pred_1h = self.cross_attn(mono_pred_1h, vox_proposal)\n",
    "        #return mono_pred_1h\n",
    "        return vox_pred_1h\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d3e247e1-fd4b-46c0-8c87-e806f6fd2f95",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters (in millions): 2.92856 M\n"
     ]
    }
   ],
   "source": [
    "model = STFB_Occ(num_classes=num_classes, \n",
    "                 sttr=sttr, \n",
    "                 monoscene=monoscene)\n",
    "print(\"Number of parameters (in millions):\", sum(p.numel() for p in model.parameters() if p.requires_grad) / 1_000_000, 'M')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "52fbc9d6-76ab-4423-be11-e9df290bb26a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.sttr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e51256b7-806b-4cd9-8568-f120b8ff1891",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68af7f18-a021-4d2c-8fc4-262b2c0644fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d89be07c-838c-49a3-b981-1781b1fe97f8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# for i, (image2, image3, voxel_labels) in tqdm(enumerate(train_dataloader),total = len(train_dataloader)):\n",
    "#     if i == 0:\n",
    "#         print(image2.shape)\n",
    "#         print(image3.shape)\n",
    "#         print(voxel_labels.shape)\n",
    "#         inputs = torch.stack((image2, image3), dim=1).to(device)\n",
    "        \n",
    "#         print(model.sttr(inputs).shape)\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec8cb76-ed64-489e-aaf5-348cc1ab019d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8f56464e-e4d3-4437-a95f-dfb0d2763c1f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1362 [00:00<?, ?it/s]/workspace/stereo-transformer/module/pos_encoder.py:56: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  dim_t = self.temperature ** (2 * (dim_t // 2) / self.num_pos_feats)\n",
      "/opt/conda/lib/python3.7/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\"None of the inputs have requires_grad=True. Gradients will be None\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 376, 1241]) torch.Size([1, 376, 1241])\n",
      "torch.Size([4, 1, 1, 47, 155])\n",
      "torch.Size([4, 1, 256, 256, 32])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/HKU-OccNet/monoscene/monoscene.py:100: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  projected_pix // scale_2d,\n",
      "/workspace/HKU-OccNet/monoscene/monoscene.py:107: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  projected_pix // scale_2d,\n",
      "  0%|          | 0/1362 [00:02<?, ?it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">╭─────────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> ────────────────────────────────╮</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;module&gt;</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">14</span>                                                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">11 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>inputs = torch.stack((image2, image3), dim=<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span>).to(device)                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">12 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>voxel_labels = voxel_labels.to(device)                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">13 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>                                                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>14 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>voxel_pred_1h = model(inputs)                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">15 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>                                                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">16 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>loss = sem_scal_loss(voxel_pred_1h, voxel_labels)                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">17 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>loss += geo_scal_loss(voxel_pred_1h, voxel_labels)                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/opt/conda/lib/python3.7/site-packages/torch/nn/modules/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">module.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1130</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_call_impl</span>             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1127 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># this function, and just call forward.</span>                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1128 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> (<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._forward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._forward_pre_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">o</span>  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1129 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_pre_hooks):                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1130 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> forward_call(*<span style=\"color: #00ffff; text-decoration-color: #00ffff\">input</span>, **kwargs)                                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1131 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># Do not call functions when jit is used</span>                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1132 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>full_backward_hooks, non_full_backward_hooks = [], []                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1133 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_backward_hooks:                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">forward</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">73</span>                                                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">70 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">#print(mono_pred_1h.shape) #[1, 20, 256, 256, 32]</span>                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">71 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># vox_propasal has shape = [255, 255, 32]</span>                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">72 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>                                                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>73 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>vox_pred_1h = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.cross_attn(mono_pred_1h, vox_proposal)                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">74 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">#return mono_pred_1h</span>                                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">75 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> vox_pred_1h                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">76 </span>                                                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/opt/conda/lib/python3.7/site-packages/torch/nn/modules/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">module.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1130</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_call_impl</span>             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1127 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># this function, and just call forward.</span>                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1128 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> (<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._forward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._forward_pre_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">o</span>  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1129 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_pre_hooks):                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1130 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> forward_call(*<span style=\"color: #00ffff; text-decoration-color: #00ffff\">input</span>, **kwargs)                                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1131 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># Do not call functions when jit is used</span>                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1132 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>full_backward_hooks, non_full_backward_hooks = [], []                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1133 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_backward_hooks:                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">forward</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">44</span>                                                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">41 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>mono_pred_1h = pred_1h                                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">42 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>                                                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">43 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># Cross-attention</span>                                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>44 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>vox_proposal_expanded = vox_proposal.unsqueeze(<span style=\"color: #0000ff; text-decoration-color: #0000ff\">0</span>).expand_as(mono_pred_1h)           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">45 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>attn_output = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.cross_attention(vox_proposal_expanded, mono_pred_1h, mono_pre    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">46 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>                                                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">47 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># Additional layers for processing</span>                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">RuntimeError: </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">expand</span><span style=\"font-weight: bold\">(</span>torch.cuda.FloatTensor<span style=\"font-weight: bold\">{[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">256</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">256</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">32</span><span style=\"font-weight: bold\">]}</span>, <span style=\"color: #808000; text-decoration-color: #808000\">size</span>=<span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">20</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">256</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">256</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">32</span><span style=\"font-weight: bold\">])</span>: the number of \n",
       "sizes provided <span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span><span style=\"font-weight: bold\">)</span> must be greater or equal to the number of dimensions in the tensor <span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">6</span><span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[31m╭─\u001b[0m\u001b[31m──────────────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m───────────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
       "\u001b[31m│\u001b[0m in \u001b[92m<module>\u001b[0m:\u001b[94m14\u001b[0m                                                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m11 \u001b[0m\u001b[2m│   │   \u001b[0minputs = torch.stack((image2, image3), dim=\u001b[94m1\u001b[0m).to(device)                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m12 \u001b[0m\u001b[2m│   │   \u001b[0mvoxel_labels = voxel_labels.to(device)                                              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m13 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m14 \u001b[2m│   │   \u001b[0mvoxel_pred_1h = model(inputs)                                                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m15 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m16 \u001b[0m\u001b[2m│   │   \u001b[0mloss = sem_scal_loss(voxel_pred_1h, voxel_labels)                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m17 \u001b[0m\u001b[2m│   │   \u001b[0mloss += geo_scal_loss(voxel_pred_1h, voxel_labels)                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/\u001b[0m\u001b[1;33mmodule.py\u001b[0m:\u001b[94m1130\u001b[0m in \u001b[92m_call_impl\u001b[0m             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1127 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# this function, and just call forward.\u001b[0m                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1128 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[95mnot\u001b[0m (\u001b[96mself\u001b[0m._backward_hooks \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m._forward_hooks \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m._forward_pre_hooks \u001b[95mo\u001b[0m  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1129 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[95mor\u001b[0m _global_forward_hooks \u001b[95mor\u001b[0m _global_forward_pre_hooks):                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1130 \u001b[2m│   │   │   \u001b[0m\u001b[94mreturn\u001b[0m forward_call(*\u001b[96minput\u001b[0m, **kwargs)                                         \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1131 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# Do not call functions when jit is used\u001b[0m                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1132 \u001b[0m\u001b[2m│   │   \u001b[0mfull_backward_hooks, non_full_backward_hooks = [], []                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1133 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[96mself\u001b[0m._backward_hooks \u001b[95mor\u001b[0m _global_backward_hooks:                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m in \u001b[92mforward\u001b[0m:\u001b[94m73\u001b[0m                                                                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m70 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m#print(mono_pred_1h.shape) #[1, 20, 256, 256, 32]\u001b[0m                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m71 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# vox_propasal has shape = [255, 255, 32]\u001b[0m                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m72 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m73 \u001b[2m│   │   \u001b[0mvox_pred_1h = \u001b[96mself\u001b[0m.cross_attn(mono_pred_1h, vox_proposal)                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m74 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m#return mono_pred_1h\u001b[0m                                                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m75 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mreturn\u001b[0m vox_pred_1h                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m76 \u001b[0m                                                                                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/\u001b[0m\u001b[1;33mmodule.py\u001b[0m:\u001b[94m1130\u001b[0m in \u001b[92m_call_impl\u001b[0m             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1127 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# this function, and just call forward.\u001b[0m                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1128 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[95mnot\u001b[0m (\u001b[96mself\u001b[0m._backward_hooks \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m._forward_hooks \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m._forward_pre_hooks \u001b[95mo\u001b[0m  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1129 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[95mor\u001b[0m _global_forward_hooks \u001b[95mor\u001b[0m _global_forward_pre_hooks):                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1130 \u001b[2m│   │   │   \u001b[0m\u001b[94mreturn\u001b[0m forward_call(*\u001b[96minput\u001b[0m, **kwargs)                                         \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1131 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# Do not call functions when jit is used\u001b[0m                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1132 \u001b[0m\u001b[2m│   │   \u001b[0mfull_backward_hooks, non_full_backward_hooks = [], []                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1133 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[96mself\u001b[0m._backward_hooks \u001b[95mor\u001b[0m _global_backward_hooks:                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m in \u001b[92mforward\u001b[0m:\u001b[94m44\u001b[0m                                                                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m41 \u001b[0m\u001b[2m│   │   \u001b[0mmono_pred_1h = pred_1h                                                              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m42 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m43 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# Cross-attention\u001b[0m                                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m44 \u001b[2m│   │   \u001b[0mvox_proposal_expanded = vox_proposal.unsqueeze(\u001b[94m0\u001b[0m).expand_as(mono_pred_1h)           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m45 \u001b[0m\u001b[2m│   │   \u001b[0mattn_output = \u001b[96mself\u001b[0m.cross_attention(vox_proposal_expanded, mono_pred_1h, mono_pre    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m46 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m47 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# Additional layers for processing\u001b[0m                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
       "\u001b[1;91mRuntimeError: \u001b[0m\u001b[1;35mexpand\u001b[0m\u001b[1m(\u001b[0mtorch.cuda.FloatTensor\u001b[1m{\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m1\u001b[0m, \u001b[1;36m4\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m256\u001b[0m, \u001b[1;36m256\u001b[0m, \u001b[1;36m32\u001b[0m\u001b[1m]\u001b[0m\u001b[1m}\u001b[0m, \u001b[33msize\u001b[0m=\u001b[1m[\u001b[0m\u001b[1;36m1\u001b[0m, \u001b[1;36m20\u001b[0m, \u001b[1;36m256\u001b[0m, \u001b[1;36m256\u001b[0m, \u001b[1;36m32\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m: the number of \n",
       "sizes provided \u001b[1m(\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1m)\u001b[0m must be greater or equal to the number of dimensions in the tensor \u001b[1m(\u001b[0m\u001b[1;36m6\u001b[0m\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from torch.optim import Adam\n",
    "import torch.nn.functional as F\n",
    "model = model.to(device)\n",
    "optimizer = Adam(model.parameters(), lr=0.01)\n",
    "num_epochs = 3\n",
    "best_loss = np.inf\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = 0.0\n",
    "    for i, (image2, image3, voxel_labels) in tqdm(enumerate(train_dataloader),total = len(train_dataloader)):\n",
    "        inputs = torch.stack((image2, image3), dim=1).to(device)\n",
    "        voxel_labels = voxel_labels.to(device)\n",
    "        \n",
    "        voxel_pred_1h = model(inputs)\n",
    "        \n",
    "        loss = sem_scal_loss(voxel_pred_1h, voxel_labels)\n",
    "        loss += geo_scal_loss(voxel_pred_1h, voxel_labels)\n",
    "        class_weights = class_weights.float().to(device)\n",
    "        loss += CE_ssc_loss(voxel_pred_1h, voxel_labels, class_weights)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "        #print(loss.item())  \n",
    "    valid_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for (image2, image3, voxel_labels) in tqdm(val_dataloader):\n",
    "            inputs = torch.stack((image2, image3), dim=1).to(device)\n",
    "            voxel_labels = voxel_labels.to(device)\n",
    "            \n",
    "            voxel_pred_1h = model(inputs)\n",
    "            \n",
    "            loss = sem_scal_loss(voxel_pred_1h, voxel_labels)\n",
    "            loss += geo_scal_loss(voxel_pred_1h, voxel_labels)\n",
    "            class_weights = class_weights.float().to(device)\n",
    "            loss += CE_ssc_loss(voxel_pred_1h, voxel_labels, class_weights)\n",
    "            \n",
    "            valid_loss += loss.item()\n",
    "            \n",
    "    print(f'Epoch {epoch + 1}: Training loss: {train_loss / len(train_dataloader)}, Validation loss: {valid_loss / len(val_dataloader)}')\n",
    "    if (train_loss / len(train_dataloader)) < best_loss:\n",
    "        torch.save(model.state_dict(), 'STFBOcc.pth')\n",
    "        best_loss = (train_loss / len(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a9a9e2c-80f7-4b1f-9690-23d0b12ca684",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e2e6cab-b480-4da7-a5fc-6cf48fae330d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96dba1f0-dcd0-4f74-a494-d24214584121",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "755f701b-b14d-4d1b-a723-4b7b43e91bed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efaa8f8b-052c-444d-ab4d-d26469d07d66",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63d45fca-f327-4d8d-897a-f46952d28f41",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a42c43b8-d1dd-4723-a207-df7e3fdea26d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "333b86de-4461-485f-a960-6aff3288f5f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcd65a6b-bfe2-40cc-a8d1-4aa0284d07af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55b84fa9-eb80-42ca-88a6-f6c684d5f248",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5f6398d-3308-4f8b-be27-2c3637b7d5e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18af660e-6e64-4719-a922-6cc68b3eb9dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "298ae512-95e6-414c-99b9-983c6ec6304a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a11aedc-29a0-4705-971a-9237f121bb29",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2239413-7da2-49c3-bbcb-1c76feb55dcd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93851041-b3b5-4694-ae7b-9c8a27ecad20",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0698e9bc-3f7c-45a8-a730-b7a00af828a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bea00931-49c5-4a6c-905a-f0607fc20e65",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23c54b15-031c-4db1-a93d-8e79e3445be3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f82c79a3-5138-4293-8232-b485f41fbf15",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1243a980-9129-44c9-a3e1-598e25f28b9b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e751c53-b21b-4274-9bad-f0871d7c48da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34857c72-f7f9-494c-bc4e-2cba89ee0850",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1994be26-09cb-4add-ad0a-6ff6593f59ac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
