{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ebcf2ec3-c8c1-4871-8c6e-5f49cba9f710",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import argparse\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import random\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchsummary import summary\n",
    "\n",
    "import bqplot.scales\n",
    "import ipyvolume as ipv\n",
    "import ipywidgets as widgets\n",
    "\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") \n",
    "print(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52bb5216-a8bf-479e-8f10-6b1d6aef4f4d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1765bb9c-f001-435a-84d3-5f756b759a9d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1089\n",
      "273\n"
     ]
    }
   ],
   "source": [
    "sys.path.append(\"/workspace/HKU-OccNet/\")\n",
    "from utils import SemanticKITTIDataset\n",
    "from utils import visualize_labeled_array3d\n",
    "from utils import plot_tensor2d\n",
    "from torch.utils.data import random_split\n",
    "from utils.monoscene_utils import *\n",
    "\n",
    "KITTI_DIR = \"/workspace/Dataset/dataset\"\n",
    "\n",
    "\n",
    "\n",
    "train_set = SemanticKITTIDataset(root_dir=KITTI_DIR, mode='train', \n",
    "                                 sequences=['00'], split_ratio=0.3)\n",
    "\n",
    "\n",
    "split_idx = int(0.8 * len(train_set))\n",
    "train_subset, val_subset = random_split(train_set, [split_idx, len(train_set) - split_idx])\n",
    "\n",
    "\n",
    "num_classes = len(train_set.class_names)\n",
    "class_weights = train_set.class_weights\n",
    "\n",
    "img_width, img_height = 1241, 376\n",
    "\n",
    "print(len(train_subset))  # 训练集子集的长度\n",
    "print(len(val_subset))    # 验证集子集的长度\n",
    "\n",
    "\n",
    "calib = read_calib(\"/workspace/HKU-OccNet/calib.txt\")\n",
    "\n",
    "#get_projections(img_width, img_height, calib)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f0d89cfe-64da-4d48-bf8a-b0836deb859c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OMP: Info #276: omp_set_nested routine deprecated, please use omp_set_max_active_levels instead.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dict_keys(['projected_pix_1', 'pix_z_1', 'fov_mask_1', 'projected_pix_2', 'pix_z_2', 'fov_mask_2'])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#get_projections(img_width, img_height, calib)['fov_mask_1'].view(256, 256, 32).shape\n",
    "\n",
    "get_projections(img_width, img_height, calib).keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b22b6967-27fc-4755-af1a-887d78113c5a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#left_img, right_img, vox_labels = train_set.get_data(666)\n",
    "# plt.figure(1)\n",
    "# plot_tensor2d(left_img)\n",
    "# plt.figure(2)\n",
    "# plot_tensor2d(right_img)\n",
    "# plt.show()\n",
    "#visualize_labeled_array3d(vox_labels.numpy().astype(np.uint16), size = 0.5, marker = 'box')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0feca197-92bb-4412-acd6-ed6f6933c462",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#left_img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e2d19b9-3544-4bd5-8cd9-c84ddb00b1fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0ae7b44d-c900-4e07-ad82-549202b5f4c0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "P_rect_02 = torch.tensor([\n",
    "    [7.215377e+02, 0.000000e+00, 6.095593e+02, 4.485728e+01],\n",
    "    [0.000000e+00, 7.215377e+02, 1.728540e+02, 2.163791e-01],\n",
    "    [0.000000e+00, 0.000000e+00, 1.000000e+00, 2.745884e-03]\n",
    "], dtype=torch.float32)\n",
    "P_rect_03 = torch.tensor([\n",
    "    [7.215377e+02, 0.000000e+00, 6.095593e+02, -3.395242e+02],\n",
    "    [0.000000e+00, 7.215377e+02, 1.728540e+02, 2.199936e+00],\n",
    "    [0.000000e+00, 0.000000e+00, 1.000000e+00, 2.729905e-03]\n",
    "], dtype=torch.float32)\n",
    "\n",
    "\n",
    "import torch\n",
    "\n",
    "def disparity_to_3D_points(disparity, left_prj=P_rect_02, right_prj=P_rect_03):\n",
    "    \"\"\"\n",
    "    Convert disparity map to 3D points using the given projection matrices.\n",
    "\n",
    "    Parameters:\n",
    "    - disparity: 2D torch tensor representing the disparity map.\n",
    "    - left_prj: 3x4 torch tensor representing the left rectified projection matrix.\n",
    "    - right_prj: 3x4 torch tensor representing the right rectified projection matrix.\n",
    "\n",
    "    Returns:\n",
    "    - 3D torch tensor with shape (H, W, 3) representing the 3D points.\n",
    "    \"\"\"\n",
    "    W, H = disparity.shape\n",
    "    f = left_prj[0, 0]  # Focal length\n",
    "    T = right_prj[0, 3] / f  # Baseline\n",
    "\n",
    "    Q = torch.tensor([\n",
    "        [1, 0, 0, -0.5 * W],\n",
    "        [0, -1, 0, 0.5 * H],\n",
    "        [0, 0, 0, -f],\n",
    "        [0, 0, -1 / T, 0]\n",
    "    ], dtype=torch.float32)\n",
    "\n",
    "    # Create meshgrid for pixel coordinates\n",
    "    u, v = torch.meshgrid(torch.arange(W, dtype=torch.float32), torch.arange(H, dtype=torch.float32))\n",
    "\n",
    "#     print(u.shape)\n",
    "#     print(v.shape)\n",
    "#     print(disparity.shape)\n",
    "    # Reproject to 3D\n",
    "    homog_coords = torch.stack((u, v, torch.ones_like(u), disparity), dim=-1)\n",
    "    points_3D = torch.matmul(homog_coords, Q.transpose(0, 1))\n",
    "    points_3D = points_3D[:, :, :3] / points_3D[:, :, 3].unsqueeze(-1)  # Convert from homogeneous to 3D coordinates\n",
    "\n",
    "    return points_3D\n",
    "\n",
    "\n",
    "#disparity_to_3D_points(disp_img.squeeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a89108ca-bc63-426c-9a29-c14e6fb33718",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96934b4a-42d1-4f23-99a4-67a8f05dcb55",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc837fc5-5391-402d-9bf5-7887550d9808",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c3f1929-ebbe-4afc-b7a0-348cd78f9df6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9827529-6450-4ef0-8317-a7ff13a79ab1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d0d5e89-c81e-47ee-aa8f-5bfe2ac64f19",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "521484c5-1fb6-45aa-bcde-01563e806ec5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae98db0e-9573-4074-8bc7-13d24f220c63",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2ba6b047-3ca8-4e40-8d82-70f3cc161d91",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/pytorch_lightning/utilities/migration/migration.py:200: PossibleUserWarning: You have multiple `ModelCheckpoint` callback states in this checkpoint, but we found state keys that would end up colliding with each other after an upgrade, which means we can't differentiate which of your checkpoint callbacks needs which states. At least one of your `ModelCheckpoint` callbacks will not be able to reload the state.\n",
      "  category=PossibleUserWarning,\n",
      "Lightning automatically upgraded your loaded checkpoint from v1.1.3 to v1.9.5. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint --file ../../PretrainedWeights/monoscene_kitti.ckpt`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_relations 4\n",
      "Loading base model ()..."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/rwightman_gen-efficientnet-pytorch_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n",
      "Removing last two layers (global_pool & classifier).\n",
      "Building Encoder-Decoder model..Done.\n",
      "Monoscene parameter: 149555444\n"
     ]
    }
   ],
   "source": [
    "from monoscene.monoscene import MonoScene\n",
    "from utils.monoscene_utils import *\n",
    "\n",
    "pretrained_weight_path = '/workspace/PretrainedWeights/monoscene_kitti.ckpt'\n",
    "\n",
    "\n",
    "monoscene_pt = MonoScene.load_from_checkpoint(\n",
    "        pretrained_weight_path,\n",
    "        dataset=\"kitti\",\n",
    "        n_classes=20,\n",
    "        feature = 64,\n",
    "        project_scale = 2,\n",
    "        full_scene_size = (256, 256, 32),\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class MonoScene(nn.Module):\n",
    "    def __init__(self, MonoScene_pretrained, calib, img_width, img_height):\n",
    "        super(MonoScene, self).__init__()\n",
    "\n",
    "        self.monoscene_pt = MonoScene_pretrained\n",
    "        self.batch_dict = get_projections(img_width, img_height, calib)\n",
    "        for key in self.batch_dict:\n",
    "            self.batch_dict[key] = self.batch_dict[key].unsqueeze(0)\n",
    "            self.batch_dict[key] = self.batch_dict[key].to(device)\n",
    "        \n",
    "    \n",
    "    # input_tensor 'x' should be batched stereo image tensor with\n",
    "    # shape: N x 2 x C x H x W, where N is the batch size, \n",
    "    # 2 for left and right images, \n",
    "    # and C,H,W are the dimensions of RGB images \n",
    "    def forward(self, x):\n",
    "        #print(x.shape)\n",
    "\n",
    "        self.batch_dict[\"img\"] = x.to(device)\n",
    "\n",
    "        x = self.monoscene_pt(self.batch_dict)\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "print(\"Monoscene parameter:\", sum(p.numel() for p in monoscene_pt.parameters() if p.requires_grad))\n",
    "for p in monoscene_pt.parameters():\n",
    "     p.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aea582f-29fe-4883-bde4-acc3cfb71d68",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1038f247-7974-4204-ab40-0ee5cf9674b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d75e1040-c9f8-4657-98f5-390b89202a69",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b62a061e-f244-4c2c-8892-049c04c581bb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-trained model successfully loaded.\n",
      "Number of parameters (in millions): 2.513811 M\n"
     ]
    }
   ],
   "source": [
    "from utils import load_STTR_model\n",
    "from utils import NestedTensor, batched_index_select\n",
    "\n",
    "\n",
    "pretrained_weight_path = '/workspace/PretrainedWeights/kitti_finetuned_model.pth.tar'\n",
    "\n",
    "sttr_pt = load_STTR_model(pretrained_weight_path)\n",
    "for param in sttr_pt.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "\n",
    "print(\"Number of parameters (in millions):\", sum(p.numel() for p in sttr_pt.parameters()) / 1_000_000, 'M')\n",
    "\n",
    "\n",
    "class STTR_InputAdapterLayer(nn.Module):\n",
    "    def __init__(self, downsample=3):\n",
    "        super(STTR_InputAdapterLayer, self).__init__()\n",
    "        self.downsample = downsample\n",
    "\n",
    "    def forward(self, input_tensor):\n",
    "        input_tensor = input_tensor\n",
    "        bs, _, _, h, w = input_tensor.shape  # Extract batch size, height, and width\n",
    "\n",
    "        # Extract left and right images from the input tensor\n",
    "        left_imgs = input_tensor[:, 0, :, :, :].squeeze(1)\n",
    "        right_imgs = input_tensor[:, 1, :, :, :].squeeze(1)\n",
    "\n",
    "        col_offset = int(self.downsample / 2)\n",
    "        row_offset = int(self.downsample / 2)\n",
    "        sampled_cols = torch.arange(col_offset, w, self.downsample)[None,].expand(bs, -1).cuda()\n",
    "        sampled_rows = torch.arange(row_offset, h, self.downsample)[None,].expand(bs, -1).cuda()\n",
    "        \n",
    "\n",
    "        # Create NestedTensor for the batch\n",
    "        nested_tensor = NestedTensor(left_imgs, right_imgs,  \n",
    "                                    sampled_cols=sampled_cols, sampled_rows=sampled_rows)\n",
    "\n",
    "        return nested_tensor\n",
    "\n",
    "class STTR(nn.Module):\n",
    "    def __init__(self, STTR_pretrained):\n",
    "        super(STTR, self).__init__()\n",
    "        self.sttr_adapter_layer = STTR_InputAdapterLayer(downsample=3)\n",
    "        self.sttr_pt = STTR_pretrained\n",
    "        \n",
    "    \n",
    "    # input_tensor 'x' should be batched stereo image tensor with\n",
    "    # shape: N x 2 x C x H x W, where N is the batch size, \n",
    "    # 2 for left and right images, \n",
    "    # and C,H,W are the dimensions of RGB images \n",
    "    def forward(self, x): \n",
    "        x = self.sttr_adapter_layer(x)\n",
    "        # bs, _, h, w = x.left.size()\n",
    "        # feat = self.sttr_pt.backbone(x)\n",
    "        # tokens = self.sttr_pt.tokenizer(feat)\n",
    "        # pos_enc = self.sttr_pt.pos_encoder(x)\n",
    "        # # separate left and right\n",
    "        # feat_left = tokens[:bs]\n",
    "        # feat_right = tokens[bs:]  # NxCxHxW\n",
    "        # # downsample\n",
    "        # if x.sampled_cols is not None:\n",
    "        #     feat_left = batched_index_select(feat_left, 3, x.sampled_cols)\n",
    "        #     feat_right = batched_index_select(feat_right, 3, x.sampled_cols)\n",
    "        # if x.sampled_rows is not None:\n",
    "        #     feat_left = batched_index_select(feat_left, 2, x.sampled_rows)\n",
    "        #     feat_right = batched_index_select(feat_right, 2, x.sampled_rows)\n",
    "        # attn_weight = self.sttr_pt.transformer(feat_left, feat_right, pos_enc)\n",
    "        # output = self.sttr_pt.regression_head(attn_weight, x)\n",
    "        output = self.sttr_pt(x)\n",
    "        disp_map = output['disp_pred'][0]\n",
    "        occ_map = output['occ_pred'][0] > 0.5\n",
    "        disp_map[occ_map] = 0.0\n",
    "        \n",
    "        return disp_map\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d29f60c3-008c-4b20-9bc1-47106f98e501",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c973efc-0e00-48f6-928b-e5e908a637bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca4270ef-45b3-4f5a-b693-467308002fa0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5135e04-2762-4b37-b60f-3f87a462a24d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f1edf26-ac82-47d6-a8e0-abec50fbffe1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d8e6948c-d95b-4622-87b5-6799ec40d509",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from utils import sem_scal_loss, geo_scal_loss, CE_ssc_loss\n",
    "from utils import Header\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c3d7dee4-9c56-4a80-ade1-87b2900d9405",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sttr = STTR(sttr_pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3c9bae4c-99c4-47eb-939f-b31852793f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "monoscene = MonoScene(monoscene_pt, calib, img_width, img_height)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9b418fe8-bd9d-4897-b984-e023f4635911",
   "metadata": {},
   "outputs": [],
   "source": [
    "# img shape: 3, 376, 1241 \n",
    "# voxel shape: 256, 256, 32\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cfced9e-e4ac-4a70-bbd1-c43312bf8163",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b49ddc4d-1b2c-4b2e-882f-84f3fa27752d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5fba344-9f7d-41f9-bf89-a1e8be1a9fff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2e68dcaa-35dc-4a44-b758-66e50d0b9bbc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CrossAttentionModule(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(CrossAttentionModule, self).__init__()\n",
    "        # Define layers for cross-attention\n",
    "        self.query_conv = nn.Conv3d(in_channels, out_channels, kernel_size=1)\n",
    "        self.key_conv = nn.Conv3d(in_channels, out_channels, kernel_size=1)\n",
    "        self.value_conv = nn.Conv3d(in_channels, out_channels, kernel_size=1)\n",
    "\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, query, key, value):\n",
    "        batch_size = query.shape[0]\n",
    "\n",
    "        # Transform query, key, value\n",
    "        query = self.query_conv(query)\n",
    "        key = self.key_conv(key)\n",
    "        value = self.value_conv(value)\n",
    "\n",
    "        # Reshape for matmul\n",
    "        query = query.view(batch_size, -1, query.shape[2]*query.shape[3]*query.shape[4])\n",
    "        key = key.view(batch_size, -1, key.shape[2]*key.shape[3]*key.shape[4]).permute(0, 2, 1)\n",
    "        value = value.view(batch_size, -1, value.shape[2]*value.shape[3]*value.shape[4])\n",
    "\n",
    "        # Attention mechanism\n",
    "        attention = torch.bmm(query, key)\n",
    "        attention = self.softmax(attention)\n",
    "        out = torch.bmm(value, attention.permute(0, 2, 1))\n",
    "\n",
    "        # Reshape back to original size\n",
    "        out = out.view(batch_size, -1, query.shape[2], query.shape[3], query.shape[4])\n",
    "        return out\n",
    "\n",
    "class VoxelCrossAttn(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(VoxelCrossAttn, self).__init__()\n",
    "        self.monoscene = ...  # Your existing MonoScene model\n",
    "        self.cross_attention = CrossAttentionModule(num_classes, num_classes)\n",
    "        self.additional_layers = ...  # Additional layers as needed\n",
    "\n",
    "    def forward(self, pred_1h, vox_proposal):\n",
    "        mono_pred_1h = pred_1h\n",
    "\n",
    "        # Cross-attention\n",
    "        vox_proposal_expanded = vox_proposal.unsqueeze(0).expand_as(mono_pred_1h)\n",
    "        attn_output = self.cross_attention(vox_proposal_expanded, mono_pred_1h, mono_pred_1h)\n",
    "\n",
    "        # Additional layers for processing\n",
    "        output = self.additional_layers(attn_output)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7daa0763-fc0a-4ac2-839d-3e254b83352c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class STFB_Occ(nn.Module):\n",
    "    def __init__(self, num_classes, sttr, monoscene):\n",
    "        super(STFB_Occ, self).__init__()\n",
    "        self.sttr = sttr\n",
    "        self.monoscene = monoscene\n",
    "        \n",
    "        #self.depth_prj_voxel = DepthProjectVoxel()\n",
    "        \n",
    "\n",
    "        # Initial convolution layers\n",
    "        self.conv1 = nn.Conv2d(4, 64, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
    "\n",
    "        # FPN Layers\n",
    "        self.toplayer = nn.Conv2d(256, 256, kernel_size=1, stride=1, padding=0)  # Reduce channels\n",
    "        self.latlayer1 = nn.Conv2d(128, 256, kernel_size=1, stride=1, padding=0)\n",
    "        self.latlayer2 = nn.Conv2d(64, 256, kernel_size=1, stride=1, padding=0)\n",
    "\n",
    "        self.smooth1 = nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1)\n",
    "        self.smooth2 = nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1)\n",
    "        \n",
    "        self.channel_reducer = nn.Conv2d(in_channels=768, out_channels=3, kernel_size=1)\n",
    "        \n",
    "        self.cross_attn = VoxelCrossAttn(num_classes)\n",
    "\n",
    "    def _upsample_add(self, x, y):\n",
    "        _, _, H, W = y.size()\n",
    "        return nn.functional.interpolate(x, size=(H, W), mode='bilinear', align_corners=False) + y\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Existing STTR depth estimation\n",
    "        x_depth = self.sttr(x)\n",
    "        x_depth = x_depth.unsqueeze(0)\n",
    "        \n",
    "\n",
    "        left_image = x[:, 0, :, :, :].squeeze(1)  # Shape: [N, C, H, W] (N always 1)\n",
    "        \n",
    "        #print(left_image.shape, x_depth.shape)\n",
    "        #vox_proposal = self.depth_prj_voxel(left_image, x_depth)\n",
    "        #print(vox_proposal.shape)\n",
    "        \n",
    "        \n",
    "        x = torch.cat((left_image, x_depth.unsqueeze(0)), dim=1)  # Shape: [N, C+1, H, W]\n",
    "\n",
    "        # Convolution layers\n",
    "        c1 = self.conv1(x)\n",
    "        c2 = self.conv2(c1)\n",
    "        c3 = self.conv3(c2)\n",
    "\n",
    "        # Top-down pathway\n",
    "        p3 = self.toplayer(c3)\n",
    "        p2 = self._upsample_add(p3, self.latlayer1(c2))\n",
    "        p1 = self._upsample_add(p2, self.latlayer2(c1))\n",
    "\n",
    "        # Smoothing\n",
    "        p2 = self.smooth1(p2)\n",
    "        p1 = self.smooth2(p1)\n",
    "\n",
    "        # Final monoscene processing\n",
    "        fused_features = torch.cat([p1, p2, p3], dim=1)\n",
    "        reduced_features = self.channel_reducer(fused_features)\n",
    "        \n",
    "        #print(fused_features.shape)\n",
    "        mono_pred_1h = self.monoscene(reduced_features) \n",
    "        # mono_pred_1h has shape [batch_size, num_classes, 255, 255, 32]\n",
    "        #print(mono_pred_1h.shape) #[1, 20, 256, 256, 32]\n",
    "        # vox_propasal has shape = [255, 255, 32]\n",
    "        \n",
    "        #vox_pred_1h = self.cross_attn(mono_pred_1h, vox_proposal)\n",
    "        return mono_pred_1h\n",
    "        #return vox_pred_1h\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d3e247e1-fd4b-46c0-8c87-e806f6fd2f95",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters (in millions): 1.670575 M\n"
     ]
    }
   ],
   "source": [
    "model = STFB_Occ(num_classes=num_classes, \n",
    "                 sttr=sttr, \n",
    "                 monoscene=monoscene)\n",
    "print(\"Number of parameters (in millions):\", sum(p.numel() for p in model.parameters() if p.requires_grad) / 1_000_000, 'M')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "52fbc9d6-76ab-4423-be11-e9df290bb26a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.sttr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e51256b7-806b-4cd9-8568-f120b8ff1891",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68af7f18-a021-4d2c-8fc4-262b2c0644fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d89be07c-838c-49a3-b981-1781b1fe97f8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# for i, (image2, image3, voxel_labels) in tqdm(enumerate(train_dataloader),total = len(train_dataloader)):\n",
    "#     if i == 0:\n",
    "#         print(image2.shape)\n",
    "#         print(image3.shape)\n",
    "#         print(voxel_labels.shape)\n",
    "#         inputs = torch.stack((image2, image3), dim=1).to(device)\n",
    "        \n",
    "#         print(model.sttr(inputs).shape)\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67643931-62c4-4756-b3b7-536bfb1fc926",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9167ef05-2259-41f8-8e10-702dbc56e264",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1089\n",
      "273\n"
     ]
    }
   ],
   "source": [
    "train_dataloader = DataLoader(train_subset, batch_size=1, shuffle=True)\n",
    "val_dataloader = DataLoader(val_subset, batch_size=1, shuffle=True)\n",
    "print(len(train_dataloader))\n",
    "print(len(val_dataloader))\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8f56464e-e4d3-4437-a95f-dfb0d2763c1f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1089 [00:00<?, ?it/s]/workspace/stereo-transformer/module/pos_encoder.py:56: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  dim_t = self.temperature ** (2 * (dim_t // 2) / self.num_pos_feats)\n",
      "/opt/conda/lib/python3.7/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\"None of the inputs have requires_grad=True. Gradients will be None\")\n",
      "/workspace/HKU-OccNet/monoscene/monoscene.py:100: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  projected_pix // scale_2d,\n",
      "/workspace/HKU-OccNet/monoscene/monoscene.py:107: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  projected_pix // scale_2d,\n",
      "100%|██████████| 1089/1089 [29:40<00:00,  1.64s/it]\n",
      "100%|██████████| 273/273 [05:28<00:00,  1.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Training loss: 9.418707533644579, Validation loss: 8.43512698407575\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1089/1089 [29:29<00:00,  1.62s/it]\n",
      "100%|██████████| 273/273 [05:27<00:00,  1.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Training loss: 7.748605572706631, Validation loss: 7.265491397389562\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1089/1089 [29:21<00:00,  1.62s/it]\n",
      "100%|██████████| 273/273 [05:27<00:00,  1.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: Training loss: 7.251076704433361, Validation loss: 7.061025791552478\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1089/1089 [29:30<00:00,  1.63s/it]\n",
      "100%|██████████| 273/273 [05:27<00:00,  1.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: Training loss: 7.054026815626356, Validation loss: 7.033551219618801\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1089/1089 [29:25<00:00,  1.62s/it]\n",
      "100%|██████████| 1089/1089 [29:36<00:00,  1.63s/it]\n",
      "100%|██████████| 273/273 [05:27<00:00,  1.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12: Training loss: 6.694172463579064, Validation loss: 6.961879373033405\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|█████▉    | 644/1089 [17:30<12:05,  1.63s/it]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">╭─────────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> ────────────────────────────────╮</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;module&gt;</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">24</span>                                                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">21 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>optimizer.zero_grad()                                                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">22 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>loss.backward()                                                                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">23 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>optimizer.step()                                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>24 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>train_loss += loss.item()                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">25 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">#print(loss.item())  </span>                                                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">26 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>valid_loss = <span style=\"color: #0000ff; text-decoration-color: #0000ff\">0.0</span>                                                                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">27 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">with</span> torch.no_grad():                                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">KeyboardInterrupt</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[31m╭─\u001b[0m\u001b[31m──────────────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m───────────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
       "\u001b[31m│\u001b[0m in \u001b[92m<module>\u001b[0m:\u001b[94m24\u001b[0m                                                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m21 \u001b[0m\u001b[2m│   │   \u001b[0moptimizer.zero_grad()                                                               \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m22 \u001b[0m\u001b[2m│   │   \u001b[0mloss.backward()                                                                     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m23 \u001b[0m\u001b[2m│   │   \u001b[0moptimizer.step()                                                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m24 \u001b[2m│   │   \u001b[0mtrain_loss += loss.item()                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m25 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m#print(loss.item())  \u001b[0m                                                               \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m26 \u001b[0m\u001b[2m│   \u001b[0mvalid_loss = \u001b[94m0.0\u001b[0m                                                                        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m27 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mwith\u001b[0m torch.no_grad():                                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
       "\u001b[1;91mKeyboardInterrupt\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from torch.optim import Adam\n",
    "import torch.nn.functional as F\n",
    "\n",
    "optimizer = Adam(model.parameters(), lr=0.0001)\n",
    "num_epochs = 20\n",
    "best_loss = np.inf\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = 0.0\n",
    "    for i, (image2, image3, voxel_labels) in tqdm(enumerate(train_dataloader),total = len(train_dataloader)):\n",
    "        inputs = torch.stack((image2, image3), dim=1).to(device)\n",
    "        voxel_labels = voxel_labels.to(device)\n",
    "        \n",
    "        voxel_pred_1h = model(inputs)\n",
    "        \n",
    "        loss = sem_scal_loss(voxel_pred_1h, voxel_labels)\n",
    "        loss += geo_scal_loss(voxel_pred_1h, voxel_labels)\n",
    "        class_weights = class_weights.float().to(device)\n",
    "        loss += CE_ssc_loss(voxel_pred_1h, voxel_labels, class_weights)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "        #print(loss.item())  \n",
    "    valid_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for (image2, image3, voxel_labels) in tqdm(val_dataloader):\n",
    "            inputs = torch.stack((image2, image3), dim=1).to(device)\n",
    "            voxel_labels = voxel_labels.to(device)\n",
    "            \n",
    "            voxel_pred_1h = model(inputs)\n",
    "            \n",
    "            loss = sem_scal_loss(voxel_pred_1h, voxel_labels)\n",
    "            loss += geo_scal_loss(voxel_pred_1h, voxel_labels)\n",
    "            class_weights = class_weights.float().to(device)\n",
    "            loss += CE_ssc_loss(voxel_pred_1h, voxel_labels, class_weights)\n",
    "            \n",
    "            valid_loss += loss.item()\n",
    "            \n",
    "    print(f'Epoch {epoch + 1}: Training loss: {train_loss / len(train_dataloader)}, Validation loss: {valid_loss / len(val_dataloader)}')\n",
    "    if (train_loss / len(train_dataloader)) < best_loss:\n",
    "        torch.save(model.state_dict(), 'STFBOcc.pth')\n",
    "        best_loss = (train_loss / len(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a9a9e2c-80f7-4b1f-9690-23d0b12ca684",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e2e6cab-b480-4da7-a5fc-6cf48fae330d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a90e3c-ca40-4540-8443-bcd1492ac7ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7dac66f-6232-4ddd-9d27-89c08c7b1277",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a0eb1b1-dda1-4fe2-928e-a67083b77ca3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "803d40aa-fea3-4c22-98c3-62b9b78d7ce1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2cbcaf6-e441-4297-ba49-22ea879d580a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0486f1f-147c-4413-9f43-ea50624ec022",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f13497f-ff5e-4e01-b54c-5e9323c7e8af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec348d36-26e3-4db9-8db2-8cd4bbaad663",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c12aeb-645f-4c28-b01b-f885ba34e50e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04c71dde-4b93-48f6-b033-49838ad886d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "755f701b-b14d-4d1b-a723-4b7b43e91bed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#device = 'cpu'\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") \n",
    "model = STFB_Occ(num_classes=num_classes, sttr=sttr, monoscene=monoscene).to(device)\n",
    "model.load_state_dict(torch.load('STFBOcc_epoch12_bad.pth', map_location=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "efaa8f8b-052c-444d-ab4d-d26469d07d66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "909"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_set = SemanticKITTIDataset(root_dir=KITTI_DIR, mode='test', \n",
    "                                 sequences=['00'], split_ratio=0.8)\n",
    "test_dataloader = DataLoader(test_set, batch_size=1, shuffle=True)\n",
    "len(test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "63d45fca-f327-4d8d-897a-f46952d28f41",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/stereo-transformer/module/pos_encoder.py:56: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  dim_t = self.temperature ** (2 * (dim_t // 2) / self.num_pos_feats)\n",
      "/opt/conda/lib/python3.7/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\"None of the inputs have requires_grad=True. Gradients will be None\")\n",
      "/workspace/HKU-OccNet/monoscene/monoscene.py:100: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  projected_pix // scale_2d,\n",
      "/workspace/HKU-OccNet/monoscene/monoscene.py:107: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  projected_pix // scale_2d,\n"
     ]
    }
   ],
   "source": [
    "left_img, right_img, voxel_labels = train_set.__getitem__(555)\n",
    "inputs = torch.stack((left_img.unsqueeze(0), right_img.unsqueeze(0)), dim=1).to(device)\n",
    "voxel_pred_1h = model(inputs)\n",
    "voxel_pred = torch.argmax(voxel_pred_1h, dim=1)[0,:,:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a42c43b8-d1dd-4723-a207-df7e3fdea26d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02cd5010d8c149e3b338b7966378ce4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Container(figure=Figure(box_center=[0.5, 0.5, 0.5], box_size=[1.0, 1.0, 1.0], camera=PerspectiveCamera(fov=45.…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "visualize_labeled_array3d(voxel_labels.numpy().astype(np.uint16), size = 0.5, marker = 'box')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d84579f5-87ad-44c7-a228-64b096a2ac29",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([256, 256, 32])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "voxel_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bcd65a6b-bfe2-40cc-a8d1-4aa0284d07af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f32b5ded28e4b47bba4c652a03fd1cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Container(figure=Figure(box_center=[0.5, 0.5, 0.5], box_size=[1.0, 1.0, 1.0], camera=PerspectiveCamera(fov=45.…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "visualize_labeled_array3d(voxel_pred.cpu().numpy().astype(np.uint16), size = 0.5, marker = 'box')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55b84fa9-eb80-42ca-88a6-f6c684d5f248",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5f6398d-3308-4f8b-be27-2c3637b7d5e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18af660e-6e64-4719-a922-6cc68b3eb9dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "298ae512-95e6-414c-99b9-983c6ec6304a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "556eb28c79f449259aed020193120957",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Container(figure=Figure(box_center=[0.5, 0.5, 0.5], box_size=[1.0, 1.0, 1.0], camera=PerspectiveCamera(fov=45.…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fov_mask1 = get_projections(img_width, img_height, calib)['fov_mask_1'].view(256, 256, 32)\n",
    "visualize_labeled_array3d(fov_mask1.numpy().astype(np.uint16), size = 0.5, marker = 'box')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a11aedc-29a0-4705-971a-9237f121bb29",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2239413-7da2-49c3-bbcb-1c76feb55dcd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93851041-b3b5-4694-ae7b-9c8a27ecad20",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0698e9bc-3f7c-45a8-a730-b7a00af828a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">╭─────────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> ────────────────────────────────╮</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;module&gt;</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">2</span>                                                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1 </span>left_img, right_img, voxel_labels = train_set.<span style=\"color: #00ff00; text-decoration-color: #00ff00\">__getitem__</span>(<span style=\"color: #0000ff; text-decoration-color: #0000ff\">100</span>)                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>2 inputs = torch.stack((left_img.unsqueeze(<span style=\"color: #0000ff; text-decoration-color: #0000ff\">0</span>).numpy(), right_img.unsqueeze(<span style=\"color: #0000ff; text-decoration-color: #0000ff\">0</span>)), dim=<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span>).to(     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">3 </span>disp_img = sttr(inputs)                                                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">4 </span>disp_img = disp_img.cpu().unsqueeze(<span style=\"color: #0000ff; text-decoration-color: #0000ff\">0</span>)                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">5 </span>                                                                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">TypeError: </span>expected Tensor as element <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span> in argument <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, but got numpy.ndarray\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[31m╭─\u001b[0m\u001b[31m──────────────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m───────────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
       "\u001b[31m│\u001b[0m in \u001b[92m<module>\u001b[0m:\u001b[94m2\u001b[0m                                                                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1 \u001b[0mleft_img, right_img, voxel_labels = train_set.\u001b[92m__getitem__\u001b[0m(\u001b[94m100\u001b[0m)                               \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m2 inputs = torch.stack((left_img.unsqueeze(\u001b[94m0\u001b[0m).numpy(), right_img.unsqueeze(\u001b[94m0\u001b[0m)), dim=\u001b[94m1\u001b[0m).to(     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m3 \u001b[0mdisp_img = sttr(inputs)                                                                      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m4 \u001b[0mdisp_img = disp_img.cpu().unsqueeze(\u001b[94m0\u001b[0m)                                                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m5 \u001b[0m                                                                                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
       "\u001b[1;91mTypeError: \u001b[0mexpected Tensor as element \u001b[1;36m0\u001b[0m in argument \u001b[1;36m0\u001b[0m, but got numpy.ndarray\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "left_img, right_img, voxel_labels = train_set.__getitem__(100)\n",
    "inputs = torch.stack((left_img.unsqueeze(0).numpy(), right_img.unsqueeze(0)), dim=1).to(device)\n",
    "disp_img = sttr(inputs)\n",
    "disp_img = disp_img.cpu().unsqueeze(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bea00931-49c5-4a6c-905a-f0607fc20e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_tensor2d(left_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f82c79a3-5138-4293-8232-b485f41fbf15",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_tensor2d(disp_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a306273-d940-4baf-a855-08baf43518e7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "disp_img.shape\n",
    "disp_img.squeeze(0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1243a980-9129-44c9-a3e1-598e25f28b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "prj3dpts = disparity_to_3D_points(disp_img.squeeze(0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e890862f-f84b-441a-b6a5-a4acbd7c43e4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prj3dpts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e751c53-b21b-4274-9bad-f0871d7c48da",
   "metadata": {},
   "outputs": [],
   "source": [
    "disp_img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34857c72-f7f9-494c-bc4e-2cba89ee0850",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1994be26-09cb-4add-ad0a-6ff6593f59ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['projected_pix_1', 'pix_z_1', 'fov_mask_1', 'projected_pix_2', 'pix_z_2', 'fov_mask_2'])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_projections(img_width, img_height, calib).keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d67f6ffa-2c08-4120-b497-4406f8c43d07",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([256, 256, 32, 2])\n",
      "torch.Size([256, 256, 32])\n",
      "torch.Size([256, 256, 32])\n"
     ]
    }
   ],
   "source": [
    "prj = get_projections(img_width, img_height, calib)\n",
    "\n",
    "print(prj['projected_pix_1'].view(256, 256, 32, 2).shape)\n",
    "print(prj['pix_z_1'].view(256, 256, 32).shape)\n",
    "print(prj['fov_mask_1'].view(256, 256, 32).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c87474ee-ae77-4749-8dd3-61d75024007b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2fe41a7-2033-4f84-a3bb-ac180d2ca451",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eabc0acf-3682-4339-b029-8b78e272b2df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffaf430a-5bd1-4ce3-a457-01712270b72e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
